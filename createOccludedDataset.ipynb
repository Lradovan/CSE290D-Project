{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb8bb579-b79e-4a69-bfe0-50a0f0dcbb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Important paths\n",
    "'''\n",
    "\n",
    "bg_img_path = '/srv/PASCAL3D+_release1.1/Images/bus_imagenet/'\n",
    "bg_anno_path = '/srv/PASCAL3D+_release1.1/Annotations/bus_imagenet/'\n",
    "occluder_path = '/srv/occluder_libs_test_small.npz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4de32c28-5bb9-468a-a85b-231c5306081f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Get a bounding box \n",
    "Takes the path to a .mat annotation file\n",
    "'''\n",
    "\n",
    "import scipy.io as sio\n",
    "\n",
    "def get_properties(path):\n",
    "    data = sio.loadmat(path)\n",
    "    width = data['record']['size'][0][0][0][0][0]\n",
    "    height = data['record']['size'][0][0][0][0][1]\n",
    "    bbox = data['record'][0, 0]['objects'][0, 0]['bbox'][0].astype(int)\n",
    "    return width[0, 0], height[0, 0], bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1c62389-e92d-470f-b1a3-86e1b646d319",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "get overlap percentage\n",
    "'''\n",
    "def overlap_ratio(occluder_bb, occludee_bb):\n",
    "\n",
    "    #top left and bottom right points\n",
    "    occluder_x1, occluder_y1, occluder_x2, occluder_y2 = occluder_bb\n",
    "    occludee_x1, occludee_y1, occludee_x2, occludee_y2 = occludee_bb\n",
    "\n",
    "    # area of the foreground object\n",
    "    occludee_area = (occludee_x2 - occludee_x1) * (occludee_y2 - occludee_y1)\n",
    "\n",
    "    # area of the background object being covered by the foreground object\n",
    "    overlap_area = max(0, min(occludee_x2, occluder_x2) - max(occludee_x1, occluder_x1)) * max(0, min(occludee_y2, occluder_y2) - max(occludee_y1, occluder_y1))\n",
    "\n",
    "    # overlap over the total background object area\n",
    "    return overlap_area / occludee_area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e96a0873-b634-49dd-a8e1-1965c12dfb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "'''\n",
    "Get a list of randomly chosen bounding boxes to occlude the background object above some threshold\n",
    "This can be improved if we know the foreground image has to be some base scale to allow for above threshold occlusion\n",
    "'''\n",
    "def get_bbox_list(bg_bbox, bg_w, bg_h, fg_w, fg_h):\n",
    "\n",
    "    bboxes = []\n",
    "\n",
    "    occludee_x1 = bg_bbox[0]\n",
    "    occludee_y1 = bg_bbox[1] \n",
    "    occludee_x2 = bg_bbox[2]\n",
    "    occludee_y2 = bg_bbox[3]\n",
    "\n",
    "    num_boxes = 20 # iterate over multiple boxes\n",
    "    overlap_threshold = .1 # ensure a minimum amount of overlap\n",
    "\n",
    "    # print(bg_bbox, fg_w, fg_h)\n",
    "    \n",
    "    for _ in range(num_boxes):\n",
    "        # top left corner of the occluder bounding box:\n",
    "\n",
    "        occluder_x1 = random.randint(max(0, occludee_x1 - fg_w), occludee_x2) # overlapping in the x-direction\n",
    "        occluder_y1 = random.randint(max(0, occludee_y1 - fg_h), occludee_y2) # overlapping in the y-direction\n",
    "\n",
    "        # TODO: Fix so doesnt exceed background image\n",
    "        \n",
    "        occluder_x2 = occluder_x1 + fg_w\n",
    "        occluder_y2 = occluder_y1 + fg_h\n",
    "\n",
    "        # Maybe this will fix sizing error??? (IT DOES)\n",
    "        if occluder_x2 > bg_w or occluder_y2 > bg_h:\n",
    "            continue\n",
    "\n",
    "        occluder_bb = [occluder_x1, occluder_y1, occluder_x2, occluder_y2]\n",
    "        occluded_ratio = overlap_ratio(occluder_bb, bg_bbox)\n",
    "\n",
    "        if occluded_ratio >= overlap_threshold:\n",
    "            bboxes.append([occluder_x1, occluder_y1, occluder_x2, occluder_y2])\n",
    "    \n",
    "    return bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5bce983-70a5-4382-b39f-f3c477f6ed37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No module 'xformers'. Proceeding without it.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/Libcom/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:258: LightningDeprecationWarning: `pytorch_lightning.utilities.distributed.rank_zero_only` has been deprecated in v1.8.1 and will be removed in v2.0.0. You can import it from `pytorch_lightning.utilities` instead.\n",
      "  rank_zero_deprecation(\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "returns the score, composite image, and compositive mask. \n",
    "num scales is the number of different foreground scales to try.\n",
    "'''\n",
    "from libcom import OPAScoreModel\n",
    "\n",
    "def get_optimal_location(fg_img, fg_mask, bg_img, bg_w, bg_h, bg_bbox, num_scales):\n",
    "\n",
    "    net = OPAScoreModel(device=0, model_type='SimOPA')\n",
    "    cache_dir = './cache'\n",
    "\n",
    "    # from libcom.fopa_heat_map.source.prepare_multi_fg_scales import prepare_multi_fg_scales\n",
    "    scaled_fg_dir, scaled_mask_dir, csv_path = prepare_multi_fg_scales(cache_dir, fg_img, fg_mask, bg_img, 16)\n",
    "\n",
    "    score = 0\n",
    "    optimal_bbox = None\n",
    "    best_fg = None\n",
    "    best_mask = None\n",
    "    best_comp = None \n",
    "    best_comp_mask = None\n",
    "    \n",
    "    # iterate over the different foreground scales\n",
    "    with open(csv_path, mode='r', newline='') as csv_file:\n",
    "        csv_reader = csv.DictReader(csv_file)\n",
    "        for row in csv_reader:\n",
    "            fg_name   = '{}_{}_{}_{}.jpg'.format(row[\"fg_name\"].split(\".\")[0],row[\"bg_name\"].split(\".\")[0],int(row[\"newWidth\"]),int(row[\"newHeight\"]))\n",
    "            mask_name = '{}_{}_{}_{}.jpg'.format(row[\"fg_name\"].split(\".\")[0],row[\"bg_name\"].split(\".\")[0],int(row[\"newWidth\"]),int(row[\"newHeight\"]))\n",
    "            scale     = row['scale']\n",
    "            fg_w = int(row['newWidth'])\n",
    "            fg_h = int(row['newHeight'])\n",
    "            \n",
    "            save_name = fg_name.split(\".\")[0] + '_' + str(scale) + '.jpg'\n",
    "            bg_img    = read_image_pil(bg_img)\n",
    "            fg_img    = read_image_pil(os.path.join(scaled_fg_dir, fg_name))\n",
    "            fg_mask   = read_mask_pil(os.path.join(scaled_mask_dir, mask_name))\n",
    "            bbox_list = get_bbox_list(bg_bbox, bg_w, bg_h, fg_w, fg_h)\n",
    "\n",
    "            for bbox in bbox_list:\n",
    "                comp, comp_mask = get_composite_image(os.path.join(scaled_fg_dir, fg_name), os.path.join(scaled_mask_dir, mask_name), bg_img, bbox)\n",
    "                bbox_score = net(comp, comp_mask)\n",
    "                if bbox_score > score:\n",
    "                    best_fg = os.path.join(scaled_fg_dir, fg_name)\n",
    "                    best_mask = os.path.join(scaled_mask_dir, mask_name)\n",
    "                    optimal_bbox = bbox\n",
    "                    best_comp = comp\n",
    "                    best_comp_mask = comp_mask\n",
    "                    score = bbox_score\n",
    "\n",
    "        return score, best_fg, best_mask, optimal_bbox, best_comp, best_comp_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "268561c1-e56f-4a6e-96ec-a111b075838b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "'''\n",
    "Get a random occluder and correspondiong mask\n",
    "'''\n",
    "def get_occluder():\n",
    "    data = np.load(occluder_path, allow_pickle=True)\n",
    "    \n",
    "    i = random.randint(0, len(data['images']))\n",
    "    print(i)\n",
    "    occ_img = f'./occluders/fg_img_{i}.jpg'\n",
    "    occ_mask = f'./occluders/fg_mask_{i}.png'\n",
    "\n",
    "    image = data['images'][i]\n",
    "    mask = data['masks'][i]\n",
    "    print(len(data['images']))\n",
    "    if mask.max() <= 1.0:  # Check if values are between 0 and 1\n",
    "        mask = (mask * 255).astype(np.uint8)  # Scale to 0-255 and convert to uint8\n",
    "    elif mask.dtype != np.uint8:  # If it's already 0-255 but not uint8\n",
    "        mask = mask.astype(np.uint8)\n",
    "        \n",
    "    cv2.imwrite(occ_img, image)\n",
    "    cv2.imwrite(occ_mask, mask)\n",
    "\n",
    "    return occ_img, occ_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd272d49-8d1e-45cc-a636-04a95d28c960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "168\n",
      "2023\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./occluders/fg_img_168.jpg', './occluders/fg_mask_168.png')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_occluder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3138f32b-47d2-44d2-9dff-51d5deb6463f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1710\n",
      "2023\n",
      "./occluders/fg_mask_1710.png ./occluders/fg_img_1710.jpg\n",
      "564\n",
      "2023\n",
      "./occluders/fg_mask_564.png ./occluders/fg_img_564.jpg\n",
      "564\n",
      "2023\n",
      "./occluders/fg_mask_564.png ./occluders/fg_img_564.jpg\n",
      "564\n",
      "2023\n",
      "./occluders/fg_mask_564.png ./occluders/fg_img_564.jpg\n",
      "564\n",
      "2023\n",
      "./occluders/fg_mask_564.png ./occluders/fg_img_564.jpg\n"
     ]
    }
   ],
   "source": [
    "from libcom import color_transfer\n",
    "from libcom.utils.process_image import *\n",
    "from libcom.utils.environment import *\n",
    "from libcom import OPAScoreModel\n",
    "from libcom import get_composite_image\n",
    "from libcom.utils.process_image import make_image_grid\n",
    "import cv2\n",
    "import csv\n",
    "from PIL import Image\n",
    "from libcom.fopa_heat_map.source.prepare_multi_fg_scales import prepare_multi_fg_scales\n",
    "from libcom import Mure_ObjectStitchModel\n",
    "from libcom import ControlComModel\n",
    "\n",
    "# for each scale, take the highest OPA score\n",
    "\n",
    "net = Mure_ObjectStitchModel(device=0, sampler='plms')\n",
    "\n",
    "for filename in os.listdir(bg_img_path)[:5]:\n",
    "    bg_img = os.path.join(bg_img_path, filename)\n",
    "    image_id = filename.split('.')[0]\n",
    "    anno_path = os.path.join(bg_anno_path, image_id + '.mat')\n",
    "    bg_w, bg_h, bg_bbox = get_properties(anno_path)\n",
    "    fg_img, fg_mask = get_occluder()\n",
    "    print(fg_mask, fg_img)\n",
    "    score, fg_img, fg_mask, bbox, comp, comp_mask = get_optimal_location(fg_img, fg_mask, bg_img, bg_w, bg_h, bg_bbox, num_scales=16)\n",
    "    if(score):\n",
    "        res, show_fg_img = net(bg_img, [fg_img], [fg_mask], bbox, sample_steps=25, num_samples=3)\n",
    "        bg_img   = draw_bbox_on_image(bg_img, bbox)\n",
    "        grid_img = make_image_grid([bg_img, fg_img, comp, comp_mask] + [res[i] for i in range(3)])\n",
    "        cv2.imwrite(f'./results/small_comp_{filename}.jpg'.format(image_id), grid_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9a55e4-662a-4975-bd90-b7d955d4ccc5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LibCom Environment",
   "language": "python",
   "name": "libcom"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
