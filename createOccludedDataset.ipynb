{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21f821bb-27fa-4292-9d86-181f90d27a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['bicycle', 'bus', 'car', 'motorbike', 'train']\n",
    "\n",
    "occlusion_ranges = {\n",
    "    \"low\": (0.2, 0.4),\n",
    "    \"medium\": (0.4, 0.6),\n",
    "    \"high\": (0.6, 0.8)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f00d29f1-e78a-4b3e-bf65-5f2da434fd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "\n",
    "def load_one_annotation(anno_path):\n",
    "    a = scipy.io.loadmat(anno_path)\n",
    "    # I added the astype int here....\n",
    "    bbox_ = a['record'][0][0][1][0][0][1][0].astype(int)\n",
    "    w = a['record']['size'][0][0][0][0][0][0][0]\n",
    "    h = a['record']['size'][0][0][0][0][1][0][0]\n",
    "    num_obj = len(a['record'][0][0][1][0])\n",
    "    return w, h, bbox_, num_obj != 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1c62389-e92d-470f-b1a3-86e1b646d319",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "get overlap percentage based on bounding boxes\n",
    "'''\n",
    "def overlap_ratio(occluder_bb, occludee_bb):\n",
    "\n",
    "    #top left and bottom right points\n",
    "    occluder_x1, occluder_y1, occluder_x2, occluder_y2 = occluder_bb\n",
    "    occludee_x1, occludee_y1, occludee_x2, occludee_y2 = occludee_bb\n",
    "\n",
    "    # area of the foreground object\n",
    "    occludee_area = (occludee_x2 - occludee_x1) * (occludee_y2 - occludee_y1)\n",
    "\n",
    "    # area of the background object being covered by the foreground object\n",
    "    overlap_area = max(0, min(occludee_x2, occluder_x2) - max(occludee_x1, occluder_x1)) * max(0, min(occludee_y2, occluder_y2) - max(occludee_y1, occluder_y1))\n",
    "\n",
    "    # overlap over the total background object area\n",
    "    return overlap_area / occludee_area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e96a0873-b634-49dd-a8e1-1965c12dfb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "\n",
    "'''\n",
    "Get a list of randomly chosen bounding boxes to occlude the background object above some threshold\n",
    "This can be improved if we know the foreground image has to be some base scale to allow for above threshold occlusion\n",
    "'''\n",
    "def get_bbox_list(bg_bbox, bg_w, bg_h, fg_w, fg_h):\n",
    "\n",
    "    random.seed(time.time())\n",
    "\n",
    "    bboxes = []\n",
    "    occludee_x1 = bg_bbox[0]\n",
    "    occludee_y1 = bg_bbox[1] \n",
    "    occludee_x2 = bg_bbox[2]\n",
    "    occludee_y2 = bg_bbox[3]\n",
    "    num_boxes = 1\n",
    "    \n",
    "    for _ in range(num_boxes):\n",
    "\n",
    "        occluder_bb = get_random_bbox(bg_bbox, bg_w, bg_h, fg_w, fg_h)\n",
    "     \n",
    "        if not occluder_bb:\n",
    "            return bboxes\n",
    "        occluded_ratio = overlap_ratio(occluder_bb, bg_bbox)\n",
    "        \n",
    "        if .20 <= occluded_ratio <= .80:\n",
    "            bboxes.append((occluded_ratio, occluder_bb))\n",
    "    \n",
    "    return bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "694caa1e-c8f5-4153-ba52-92ab5bce3e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a random bounding box, with some unkown level of occlusion\n",
    "def get_random_bbox(bg_bbox, bg_w, bg_h, fg_w, fg_h):\n",
    "    \n",
    "    occludee_x1 = bg_bbox[0]\n",
    "    occludee_y1 = bg_bbox[1] \n",
    "    occludee_x2 = bg_bbox[2]\n",
    "    occludee_y2 = bg_bbox[3]\n",
    "\n",
    "    for _ in range(20):\n",
    "        \n",
    "        random.seed(time.time())\n",
    "        \n",
    "        occluder_x1 = random.randint(max(0, occludee_x1 - fg_w), occludee_x2) # overlapping in the x-direction\n",
    "        occluder_y1 = random.randint(max(0, occludee_y1 - fg_h), occludee_y2) # overlapping in the y-direction\n",
    "\n",
    "        # TODO: Fix so doesnt exceed background image\n",
    "        occluder_x2 = occluder_x1 + fg_w\n",
    "        occluder_y2 = occluder_y1 + fg_h\n",
    "        \n",
    "        # maybe we should introduce image cropping/clipping here instead\n",
    "        if occluder_x2 <= bg_w and occluder_y2 <= bg_h:\n",
    "            return [occluder_x1, occluder_y1, occluder_x2, occluder_y2]\n",
    "        \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61bf5e68-e42e-4870-a0be-0c128e349b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No module 'xformers'. Proceeding without it.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/Libcom/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:258: LightningDeprecationWarning: `pytorch_lightning.utilities.distributed.rank_zero_only` has been deprecated in v1.8.1 and will be removed in v2.0.0. You can import it from `pytorch_lightning.utilities` instead.\n",
      "  rank_zero_deprecation(\n"
     ]
    }
   ],
   "source": [
    "from libcom.fopa_heat_map.source.prepare_multi_fg_scales import prepare_multi_fg_scales\n",
    "\n",
    "'''\n",
    "Get a random bounding box location for a given occlusion range\n",
    "'''\n",
    "\n",
    "def get_random_location(fg_img, fg_mask, bg_img, bg_w, bg_h, bg_bbox, occ_range):\n",
    "    \n",
    "    cache_dir = './unrealistic_cache'\n",
    "    scaled_fg_dir, scaled_mask_dir, csv_path = prepare_multi_fg_scales(cache_dir, fg_img, fg_mask, bg_img, 16)\n",
    "    \n",
    "    with open(csv_path, mode='r', newline='') as csv_file:\n",
    "        csv_reader = csv.DictReader(csv_file)\n",
    "        for row in csv_reader:\n",
    "            fg_name   = '{}_{}_{}_{}.jpg'.format(row[\"fg_name\"].split(\".\")[0],row[\"bg_name\"].split(\".\")[0],int(row[\"newWidth\"]),int(row[\"newHeight\"]))\n",
    "            mask_name = '{}_{}_{}_{}.jpg'.format(row[\"fg_name\"].split(\".\")[0],row[\"bg_name\"].split(\".\")[0],int(row[\"newWidth\"]),int(row[\"newHeight\"]))\n",
    "            fg_w = int(row['newWidth'])\n",
    "            fg_h = int(row['newHeight'])\n",
    "            \n",
    "            fg_img_path = os.path.join(scaled_fg_dir, fg_name)\n",
    "            fg_mask_path = os.path.join(scaled_mask_dir, mask_name)\n",
    "\n",
    "            occ_min, occ_max = occ_range\n",
    "            # this tends to run infinitely\n",
    "            for _ in range(20):\n",
    "                occluder_bb = get_random_bbox(bg_bbox, bg_w, bg_h, fg_w, fg_h)\n",
    "                if not occluder_bb:\n",
    "                    continue\n",
    "                # we need to try a different occluder size\n",
    "                occluded_ratio = overlap_ratio(occluder_bb, bg_bbox) \n",
    "                if occ_min <= occluded_ratio <= occ_max:\n",
    "                    return fg_img_path, fg_mask_path, occluded_ratio, occluder_bb\n",
    "    \n",
    "    return None, None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5bce983-70a5-4382-b39f-f3c477f6ed37",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "returns the score, composite image, and compositive mask. \n",
    "num scales is the number of different foreground scales to try.\n",
    "'''\n",
    "from libcom import OPAScoreModel\n",
    "from libcom.fopa_heat_map.source.prepare_multi_fg_scales import prepare_multi_fg_scales\n",
    "\n",
    "def get_optimal_location(fg_img, fg_mask, bg_img, bg_w, bg_h, bg_bbox, num_scales):\n",
    "\n",
    "    net = OPAScoreModel(device=0, model_type='SimOPA')\n",
    "    cache_dir = './realistic_cache'\n",
    "\n",
    "    scaled_fg_dir, scaled_mask_dir, csv_path = prepare_multi_fg_scales(cache_dir, fg_img, fg_mask, bg_img, 16)\n",
    "\n",
    "    score = 0\n",
    "    ratio = 0\n",
    "    optimal_bbox = None\n",
    "    best_fg = None\n",
    "    best_mask = None\n",
    "    best_comp = None \n",
    "    best_comp_mask = None\n",
    "    \n",
    "    with open(csv_path, mode='r', newline='') as csv_file:\n",
    "        csv_reader = csv.DictReader(csv_file)\n",
    "        for row in csv_reader:\n",
    "            fg_name   = '{}_{}_{}_{}.jpg'.format(row[\"fg_name\"].split(\".\")[0],row[\"bg_name\"].split(\".\")[0],int(row[\"newWidth\"]),int(row[\"newHeight\"]))\n",
    "            mask_name = '{}_{}_{}_{}.jpg'.format(row[\"fg_name\"].split(\".\")[0],row[\"bg_name\"].split(\".\")[0],int(row[\"newWidth\"]),int(row[\"newHeight\"]))\n",
    "            fg_w = int(row['newWidth'])\n",
    "            fg_h = int(row['newHeight'])\n",
    "\n",
    "            bg_img    = read_image_pil(bg_img)\n",
    "            bbox_list = get_bbox_list(bg_bbox, bg_w, bg_h, fg_w, fg_h)\n",
    "\n",
    "            fg_img = os.path.join(scaled_fg_dir, fg_name)\n",
    "            fg_mask = os.path.join(scaled_mask_dir, mask_name)\n",
    "\n",
    "            for occ_ratio, bbox in bbox_list:\n",
    "                comp, comp_mask = get_composite_image(fg_img, fg_mask, bg_img, bbox)\n",
    "                bbox_score = net(comp, comp_mask)\n",
    "                if bbox_score > score:\n",
    "                    best_fg = fg_img\n",
    "                    best_mask = fg_mask\n",
    "                    optimal_bbox = bbox\n",
    "                    best_comp = comp\n",
    "                    best_comp_mask = comp_mask\n",
    "                    score = bbox_score\n",
    "                    ratio = occ_ratio\n",
    "\n",
    "        return score, ratio, best_fg, best_mask, optimal_bbox, best_comp, best_comp_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "268561c1-e56f-4a6e-96ec-a111b075838b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import time\n",
    "\n",
    "'''\n",
    "Get a random occluder and correspondiong mask for a given occlusion range\n",
    "'''\n",
    "def get_occluder(occ_size, index=None):\n",
    "    random.seed(time.time())\n",
    "    \n",
    "    # get the correct path to the occluder\n",
    "    occluder_path = '/srv/occluder_libs_test_' + occ_size + '.npz'\n",
    "    \n",
    "    # load the occluders\n",
    "    data = np.load(occluder_path, allow_pickle=True)\n",
    "    \n",
    "    if not index:\n",
    "        index = random.randint(0, len(data['images']))\n",
    "\n",
    "    # save the occluders\n",
    "    os.makedirs('./occluders', exist_ok=True)\n",
    "    occ_img = f'./occluders/fg_img_{index}.jpg'\n",
    "    occ_mask = f'./occluders/fg_mask_{index}.png'\n",
    "\n",
    "    image = data['images'][index]\n",
    "    box = data['boxes'][index]\n",
    "    mask = data['masks'][index]\n",
    "    \n",
    "    mask = (mask * 255)\n",
    "        \n",
    "    cv2.imwrite(occ_img, image)\n",
    "    cv2.imwrite(occ_mask, mask)\n",
    "    \n",
    "    h = box[1] - box[0]\n",
    "    w = box[3] - box[2]\n",
    "\n",
    "    return occ_img, occ_mask, w, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55accc2a-f490-4107-82b7-da95b910d323",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "'''\n",
    "Write to an annotation file\n",
    "'''\n",
    "\n",
    "def write_one_annotation(save_path, bg_img, cate, occ_size, occlusion_level, occlusion_ratio, occluder_path, fg_bb, bg_bb):\n",
    "    annotation = {}\n",
    "    annotation['box'] = bg_bb.tolist() \n",
    "    annotation['ratio'] = occlusion_ratio\n",
    "    annotation['occluder_box'] = fg_bb\n",
    "    annotation['occluder_path'] = occluder_path\n",
    "    annotation['source'] = bg_img\n",
    "    annotation['cate'] = cate\n",
    "    \n",
    "    img_id = bg_img.split('/')[-1].split('.')[0]\n",
    "    \n",
    "    # Convert and write JSON object to file\n",
    "    with open(f'{save_path}/annotations/{occlusion_level}/{occ_size}_{img_id}.json', \"w\") as outfile: \n",
    "        json.dump(annotation, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3138f32b-47d2-44d2-9dff-51d5deb6463f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from libcom import color_transfer\n",
    "from libcom.utils.process_image import *\n",
    "from libcom.utils.environment import *\n",
    "from libcom import OPAScoreModel\n",
    "from libcom import get_composite_image\n",
    "from libcom.utils.process_image import make_image_grid\n",
    "import cv2\n",
    "import csv\n",
    "from PIL import Image\n",
    "from libcom import Mure_ObjectStitchModel\n",
    "import json\n",
    "\n",
    "def generate_images(cate, bg_img_path, bg_w, bg_h, bg_bbox, fg_indices, save_real, save_unreal, record_file):\n",
    "\n",
    "    net = Mure_ObjectStitchModel(device=0, sampler='plms')\n",
    "    annotation = {}\n",
    "    print(bg_img_path)\n",
    "    \n",
    "    for (occ_size, fg_index) in fg_indices:\n",
    "        \n",
    "        real_fg_img_path, real_fg_mask_path, _, _ = get_occluder(occ_size, fg_index)\n",
    "\n",
    "        img_id = bg_img_path.split('/')[-1].split('.')[0]\n",
    "     \n",
    "        score, real_ratio, op_fg_img_path, op_fg_mask_path, op_bbox, _, _ = get_optimal_location(real_fg_img_path, real_fg_mask_path, bg_img_path, bg_w, bg_h, bg_bbox, num_scales=16)\n",
    "    \n",
    "        if not score:\n",
    "            record_file.write(f'Skipped {occ_size} for {bg_img_path}. Couldnt find optimal location.')\n",
    "            # we couldnt find a possible foreground location, so we just skip everything\n",
    "            continue\n",
    "\n",
    "        # write to a directory based on the level of occlusion\n",
    "        occlusion_level = None\n",
    "        for range_label, (min_occ, max_occ) in occlusion_ranges.items():\n",
    "            # print(min_occ, max_occ, real_ratio)\n",
    "            if min_occ <= real_ratio <= max_occ:\n",
    "                occlusion_level = range_label\n",
    "\n",
    "        # first, we get a random occluder (the occluder size is based off the occlusion level\n",
    "        unreal_fg_img_path, unreal_fg_mask_path, fg_w, fg_h = get_occluder(occ_size)\n",
    "\n",
    "        # get a bounding box and ratio for an occlusion that falls into the bin\n",
    "        random_fg_img_path, random_fg_mask_path, unreal_ratio, random_bbox = get_random_location(unreal_fg_img_path, unreal_fg_mask_path, bg_img_path, bg_w, bg_h, bg_bbox, occlusion_ranges[occlusion_level])\n",
    "        \n",
    "        if not random_fg_img_path:\n",
    "            record_file.write(f'Skipped {occ_size} for {bg_img_path}. Couldnt find random location/')\n",
    "             # we couldnt find a reasonable foreground, so we skip writing out anything for both foreground and background\n",
    "            continue\n",
    "            \n",
    "        # realistic image\n",
    "        write_one_annotation(save_real, bg_img_path, cate, occ_size, occlusion_level, real_ratio, real_fg_img_path, op_bbox, bg_bbox)\n",
    "        res, _ = net(bg_img_path, [op_fg_img_path], [op_fg_mask_path], op_bbox, sample_steps=25, num_samples=3)\n",
    "        # TODO: find the best result in res, instead of just taking the last sample\n",
    "        cv2.imwrite(f'{save_real}/images/{occlusion_level}/{occ_size}_{img_id}.jpg', res[2])\n",
    "        \n",
    "        # unrealistic image\n",
    "        write_one_annotation(save_unreal, bg_img_path, cate, occ_size, occlusion_level, unreal_ratio, unreal_fg_img_path, random_bbox, bg_bbox)\n",
    "        comp, _ = get_composite_image(random_fg_img_path, random_fg_mask_path, bg_img_path, random_bbox)\n",
    "        cv2.imwrite(f'{save_unreal}/images/{occlusion_level}/{occ_size}_{img_id}.jpg', comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6b12fc27-ed49-4054-b70e-f934f343fecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# # the only reason we have bg_mask_dir is in case we need it for segmentation masks...\n",
    "# def generate_images(cate, file_list, bg_img_dir, bg_anno_dir, real_save, unreal_save, record_file):\n",
    "    \n",
    "#     # set up all of the necessary directories!\n",
    "#     for dataset in [real_save, unreal_save]:\n",
    "#         for data_type in ['images', 'annotations']:\n",
    "#             for range_label in occlusion_ranges.keys():\n",
    "#                 os.makedirs(dataset + \"/\" + data_type + \"/\" + range_label, exist_ok=True)\n",
    "    \n",
    "#     for file_name in file_list[9:10]:\n",
    "#         bg_w, bg_h, bg_bbox, flag_ = load_one_annotation(os.path.join(bg_anno_dir, file_name + '.mat'))\n",
    "\n",
    "#         if flag_:\n",
    "#             record_file.write('Skipped %s for multi objects\\n' % file_name)\n",
    "#             continue\n",
    "\n",
    "#         bg_img = os.path.join(bg_img_dir, file_name + '.JPEG')\n",
    "        \n",
    "#         generate_composite_image(cate, bg_img, bg_w, bg_h, bg_bbox, real_save, unreal_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1a9a55e4-662a-4975-bd90-b7d955d4ccc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for cate in categories:\n",
    "#     print('Start cate: ', cate)\n",
    "#     tem = open('generating_record_%s_1030.txt' % cate, 'w')\n",
    "#     file_list_ = open(bg_list_path % cate).readlines()\n",
    "#     file_list_ = [tem.strip('\\n') for tem in file_list_]\n",
    "#     bg_img_path_ = bg_img_path % cate\n",
    "#     bg_anno_path_ = bg_anno_path % cate\n",
    "\n",
    "#     generate_dataset(cate, file_list_, bg_img_path_, bg_anno_path_, real_save, unreal_save, tem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "895fbb0d-8c87-48b6-b2f4-db3d6449a8f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "'''\n",
    "Important paths\n",
    "'''\n",
    "path_to_original_pascal3dp = '/srv/PASCAL3D+_release1.1/'\n",
    " \n",
    "small_occluder_path = '/srv/occluder_libs_test_small.npz'\n",
    "medium_occluder_path = '/srv/occluder_libs_test_medium.npz'\n",
    "large_occluder_path = '/srv/occluder_libs_test_large.npz'\n",
    "\n",
    "# bg_list_path = path_to_original_pascal3dp + 'Image_sets/%s_imagenet_val.txt'\n",
    "bg_img_dir = path_to_original_pascal3dp + 'Images/'\n",
    "bg_anno_dir = path_to_original_pascal3dp + 'Annotations/'\n",
    "bg_mask_dir = path_to_original_pascal3dp + 'obj_mask/'\n",
    "\n",
    "real_save = './realistic'\n",
    "unreal_save = './unrealistic'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7519977c-a1f4-42aa-b6f0-7291a4978caa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Set up the necessary directories\n",
    "'''\n",
    "for dataset in [real_save, unreal_save]:\n",
    "    for data_type in ['images', 'annotations']:\n",
    "        for range_label in occlusion_ranges.keys():\n",
    "            os.makedirs(dataset + \"/\" + data_type + \"/\" + range_label, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b3f68b-b0c2-4b59-997f-23db0dd50952",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/srv/PASCAL3D+_release1.1/Images/bicycle_imagenet/n02834778_10027.JPEG\n",
      "/srv/PASCAL3D+_release1.1/Images/bicycle_imagenet/n02834778_10155.JPEG\n",
      "/srv/PASCAL3D+_release1.1/Images/bicycle_imagenet/n02834778_10164.JPEG\n"
     ]
    }
   ],
   "source": [
    "path_to_original_pascal3dp = '/srv/PASCAL3D+_release1.1/'\n",
    "bg_anno_dir = path_to_original_pascal3dp + 'Annotations/'\n",
    "csv_path = './bg_fg_pair.csv'\n",
    "real_save = './realistic'\n",
    "record_file = open('generating_record_.txt', 'w')\n",
    "\n",
    "with open(csv_path, mode='r', newline='') as csv_file:\n",
    "    csv_reader = csv.DictReader(csv_file)\n",
    "    for row in csv_reader:\n",
    "        \n",
    "        bg_img_path = row['image_name']\n",
    "        fg_index_small = int(row['fg_index_small'])\n",
    "        fg_index_medium = int(row['fg_index_medium'])\n",
    "        fg_index_large = int(row['fg_index_large'])\n",
    "        \n",
    "        fg_indices = ((\"small\", fg_index_small), (\"medium\", fg_index_medium), (\"large\", fg_index_large))\n",
    "        \n",
    "        category = bg_img_path.split('/')[4].split('_')[0]\n",
    "        img_id = bg_img_path.split('/')[-1].split('.')[0]\n",
    "        \n",
    "        bg_anno_path = os.path.join(bg_anno_dir, category + '_imagenet', img_id + '.mat')\n",
    "        \n",
    "        # load the annotation file for the background img\n",
    "        bg_w, bg_h, bg_bbox, flag_ = load_one_annotation(bg_anno_path)\n",
    "        \n",
    "        if flag_:\n",
    "            record_file.write('Skipped %s for multi objects\\n' % file_name)\n",
    "            continue\n",
    "            \n",
    "        generate_images(category, bg_img_path, bg_w, bg_h, bg_bbox, fg_indices, real_save, unreal_save, record_file)\n",
    "        \n",
    "record_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ffe160-a3b7-4f0c-bc7a-782293ae10e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LibCom Environment (Local)",
   "language": "python",
   "name": "libcom"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
