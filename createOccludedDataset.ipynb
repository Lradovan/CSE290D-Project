{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb8bb579-b79e-4a69-bfe0-50a0f0dcbb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Important paths\n",
    "'''\n",
    "path_to_original_pascal3dp = '/srv/PASCAL3D+_release1.1/'\n",
    "# bg_img_path = '/srv/PASCAL3D+_release1.1/Images/bus_imagenet/'\n",
    "# bg_mask_path = '/srv/obj_mask/bus'\n",
    "# bg_anno_path = '/srv/PASCAL3D+_release1.1/Annotations/bus_imagenet/'\n",
    "\n",
    "occluder_path = '/srv/occluder_libs_test_small.npz'\n",
    "\n",
    "bg_list_path = path_to_original_pascal3dp + 'Image_sets/%s_imagenet_val.txt'\n",
    "bg_img_path = path_to_original_pascal3dp + 'Images/%s_imagenet'\n",
    "bg_anno_path = path_to_original_pascal3dp + 'Annotations/%s_imagenet'\n",
    "bg_mask_path = path_to_original_pascal3dp + 'obj_mask/%s'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8b0c5a3-814f-4179-bf7d-7d7051666ea0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nchange occlusion percentage to use bounding boxes\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "change occlusion percentage to use bounding boxes\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4de32c28-5bb9-468a-a85b-231c5306081f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# Get a bounding box for the occluder\n",
    "# Takes the path to a .mat annotation file\n",
    "# '''\n",
    "\n",
    "def get_properties(path):\n",
    "    data = scipy.io.loadmat(path)\n",
    "    width = data['record']['size'][0][0][0][0][0]\n",
    "    height = data['record']['size'][0][0][0][0][1]\n",
    "    bbox = data['record'][0, 0]['objects'][0, 0]['bbox'][0].astype(int)\n",
    "    return width[0, 0], height[0, 0], bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f00d29f1-e78a-4b3e-bf65-5f2da434fd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "\n",
    "def load_one_annotation(anno_path):\n",
    "    a = scipy.io.loadmat(anno_path)\n",
    "    # I added the astype int here....\n",
    "    bbox_ = a['record'][0][0][1][0][0][1][0].astype(int)\n",
    "    w = a['record']['size'][0][0][0][0][0][0][0]\n",
    "    h = a['record']['size'][0][0][0][0][1][0][0]\n",
    "    num_obj = len(a['record'][0][0][1][0])\n",
    "    return w, h, bbox_, num_obj != 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68a15888-2aa5-4133-8980-b46f27d41be1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 333, array([ 76,  28, 371, 307]), False)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_one_annotation('/srv/PASCAL3D+_release1.1/Annotations/bus_imagenet/n02924116_51203.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1c62389-e92d-470f-b1a3-86e1b646d319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS NEEDS TO BE CHANGED TO SEGMENTATION MASK PERCENTAGES\n",
    "\n",
    "'''\n",
    "get overlap percentage\n",
    "'''\n",
    "def overlap_ratio(occluder_bb, occludee_bb):\n",
    "\n",
    "    #top left and bottom right points\n",
    "    occluder_x1, occluder_y1, occluder_x2, occluder_y2 = occluder_bb\n",
    "    occludee_x1, occludee_y1, occludee_x2, occludee_y2 = occludee_bb\n",
    "\n",
    "    # area of the foreground object\n",
    "    occludee_area = (occludee_x2 - occludee_x1) * (occludee_y2 - occludee_y1)\n",
    "\n",
    "    # area of the background object being covered by the foreground object\n",
    "    overlap_area = max(0, min(occludee_x2, occluder_x2) - max(occludee_x1, occluder_x1)) * max(0, min(occludee_y2, occluder_y2) - max(occludee_y1, occluder_y1))\n",
    "\n",
    "    # overlap over the total background object area\n",
    "    return overlap_area / occludee_area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dbe5d260-c55e-4051-8d3b-1a9189e26d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "overlap percentage using segmentation masks instead of bounding boxes\n",
    "mask_map is the results of mix_masks!\n",
    "'''\n",
    "\n",
    "def check_occ_ratio_seg(mask_map, mask_obj):\n",
    "    mask_obj = mask_obj > 10\n",
    "    mask_map = mask_map > 0.5\n",
    "    in_box_size = np.sum(mask_obj)\n",
    "    in_box_value = np.sum(np.logical_and(mask_obj, mask_map))\n",
    "\n",
    "    # out_box_size = np.sum(np.logical_not(mask_obj))\n",
    "    # out_box_value = np.sum(np.logical_and(np.logical_not(mask_obj), mask_map))\n",
    "\n",
    "    return in_box_value / in_box_size #, out_box_value / out_box_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e2e8b45-f6f4-4c6f-942f-a5cbab7df105",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Place the masks into their appropriate location  in the background\n",
    "\n",
    "Tne returned mask will have 0s or 1s as values\n",
    "\n",
    "THis is not done yet....\n",
    "'''\n",
    "def mix_mask(mask, bbox, bg_w, bg_h):\n",
    "\n",
    "    background_mask = np.zeros((bg_h, bg_w), dtype=mask.dtype)\n",
    "    \n",
    "    # Extract bbox coordinates\n",
    "    x1, y1, x2, y2 = bbox\n",
    "    \n",
    "    # Ensure the mask matches the bbox dimensions\n",
    "    # if mask.shape[0] != h or mask.shape[1] != w:\n",
    "    #     mask = cv2.resize(mask, (w, h))\n",
    "    \n",
    "    # Calculate corresponding mask coordinates\n",
    "    mask_x1 = x1 - x\n",
    "    mask_y1 = y1 - y\n",
    "    mask_x2 = mask_x1 + (x2 - x1)\n",
    "    mask_y2 = mask_y1 + (y2 - y1)\n",
    "    \n",
    "    # Place the mask in the background\n",
    "    background_mask[y1:y2, x1:x2] = mask[mask_y1:mask_y2, mask_x1:mask_x2]\n",
    "    \n",
    "    return background_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e96a0873-b634-49dd-a8e1-1965c12dfb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "'''\n",
    "Get a list of randomly chosen bounding boxes to occlude the background object above some threshold\n",
    "This can be improved if we know the foreground image has to be some base scale to allow for above threshold occlusion\n",
    "'''\n",
    "def get_bbox_list(bg_bbox, bg_mask, fg_mask, bg_w, bg_h, fg_w, fg_h):\n",
    "\n",
    "    bboxes = []\n",
    "\n",
    "    occludee_x1 = bg_bbox[0]\n",
    "    occludee_y1 = bg_bbox[1] \n",
    "    occludee_x2 = bg_bbox[2]\n",
    "    occludee_y2 = bg_bbox[3]\n",
    "\n",
    "    num_boxes = 20 # iterate over multiple boxes\n",
    "    overlap_threshold = .1 # ensure a minimum amount of overlap\n",
    "\n",
    "    # print(bg_bbox, fg_w, fg_h)\n",
    "    \n",
    "    for _ in range(num_boxes):\n",
    "        # top left corner of the occluder bounding box:\n",
    "\n",
    "        occluder_x1 = random.randint(max(0, occludee_x1 - fg_w), occludee_x2) # overlapping in the x-direction\n",
    "        occluder_y1 = random.randint(max(0, occludee_y1 - fg_h), occludee_y2) # overlapping in the y-direction\n",
    "\n",
    "        # TODO: Fix so doesnt exceed background image\n",
    "        \n",
    "        occluder_x2 = occluder_x1 + fg_w\n",
    "        occluder_y2 = occluder_y1 + fg_h\n",
    "\n",
    "        # Maybe this will fix sizing error??? (IT DOES)\n",
    "        if occluder_x2 > bg_w or occluder_y2 > bg_h:\n",
    "            continue\n",
    "\n",
    "        occluder_bb = [occluder_x1, occluder_y1, occluder_x2, occluder_y2]\n",
    "    \n",
    "        #mask_map = mix_mask(fg_mask, occluder_bb, bg_w, bg_h)\n",
    "\n",
    "        occluded_ratio = overlap_ratio(occluder_bb, bg_bbox)\n",
    "        #occluded_ratio = check_occ_ratio_seg(mask_map, bg_mask)\n",
    "\n",
    "        if occluded_ratio >= overlap_threshold:\n",
    "            bboxes.append([occluder_x1, occluder_y1, occluder_x2, occluder_y2])\n",
    "    \n",
    "    return bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5bce983-70a5-4382-b39f-f3c477f6ed37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No module 'xformers'. Proceeding without it.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/Libcom/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:258: LightningDeprecationWarning: `pytorch_lightning.utilities.distributed.rank_zero_only` has been deprecated in v1.8.1 and will be removed in v2.0.0. You can import it from `pytorch_lightning.utilities` instead.\n",
      "  rank_zero_deprecation(\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "returns the score, composite image, and compositive mask. \n",
    "num scales is the number of different foreground scales to try.\n",
    "'''\n",
    "from libcom import OPAScoreModel\n",
    "\n",
    "def get_optimal_location(fg_img, fg_mask, bg_img, bg_mask, bg_w, bg_h, bg_bbox, num_scales):\n",
    "\n",
    "    net = OPAScoreModel(device=0, model_type='SimOPA')\n",
    "    cache_dir = './cache'\n",
    "\n",
    "    # from libcom.fopa_heat_map.source.prepare_multi_fg_scales import prepare_multi_fg_scales\n",
    "    scaled_fg_dir, scaled_mask_dir, csv_path = prepare_multi_fg_scales(cache_dir, fg_img, fg_mask, bg_img, 16)\n",
    "\n",
    "    score = 0\n",
    "    optimal_bbox = None\n",
    "    best_fg = None\n",
    "    best_mask = None\n",
    "    best_comp = None \n",
    "    best_comp_mask = None\n",
    "    \n",
    "    # iterate over the different foreground scales\n",
    "    with open(csv_path, mode='r', newline='') as csv_file:\n",
    "        csv_reader = csv.DictReader(csv_file)\n",
    "        for row in csv_reader:\n",
    "            fg_name   = '{}_{}_{}_{}.jpg'.format(row[\"fg_name\"].split(\".\")[0],row[\"bg_name\"].split(\".\")[0],int(row[\"newWidth\"]),int(row[\"newHeight\"]))\n",
    "            mask_name = '{}_{}_{}_{}.jpg'.format(row[\"fg_name\"].split(\".\")[0],row[\"bg_name\"].split(\".\")[0],int(row[\"newWidth\"]),int(row[\"newHeight\"]))\n",
    "            scale     = row['scale']\n",
    "            fg_w = int(row['newWidth'])\n",
    "            fg_h = int(row['newHeight'])\n",
    "            \n",
    "            save_name = fg_name.split(\".\")[0] + '_' + str(scale) + '.jpg'\n",
    "            bg_img    = read_image_pil(bg_img)\n",
    "            # these lines might not be necessary\n",
    "            # fg_img    = read_image_pil(os.path.join(scaled_fg_dir, fg_name))\n",
    "            fg_mask   = np.array(Image.open(os.path.join(scaled_mask_dir, mask_name)))\n",
    "            bbox_list = get_bbox_list(bg_bbox, bg_mask, fg_mask, bg_w, bg_h, fg_w, fg_h)\n",
    "\n",
    "            for bbox in bbox_list:\n",
    "                comp, comp_mask = get_composite_image(os.path.join(scaled_fg_dir, fg_name), os.path.join(scaled_mask_dir, mask_name), bg_img, bbox)\n",
    "                bbox_score = net(comp, comp_mask)\n",
    "                if bbox_score > score:\n",
    "                    best_fg = os.path.join(scaled_fg_dir, fg_name)\n",
    "                    best_mask = os.path.join(scaled_mask_dir, mask_name)\n",
    "                    optimal_bbox = bbox\n",
    "                    best_comp = comp\n",
    "                    best_comp_mask = comp_mask\n",
    "                    score = bbox_score\n",
    "\n",
    "        return score, best_fg, best_mask, optimal_bbox, best_comp, best_comp_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "268561c1-e56f-4a6e-96ec-a111b075838b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "'''\n",
    "Get a random occluder and correspondiong mask\n",
    "'''\n",
    "def get_occluder():\n",
    "    data = np.load(occluder_path, allow_pickle=True)\n",
    "    \n",
    "    i = random.randint(0, len(data['images']))\n",
    "    occ_img = f'./occluders/fg_img_{i}.jpg'\n",
    "    occ_mask = f'./occluders/fg_mask_{i}.png'\n",
    "\n",
    "    image = data['images'][i]\n",
    "    mask = data['masks'][i]\n",
    "    if mask.max() <= 1.0:  # Check if values are between 0 and 1\n",
    "        mask = (mask * 255).astype(np.uint8)  # Scale to 0-255 and convert to uint8\n",
    "    elif mask.dtype != np.uint8:  # If it's already 0-255 but not uint8\n",
    "        mask = mask.astype(np.uint8)\n",
    "        \n",
    "    cv2.imwrite(occ_img, image)\n",
    "    cv2.imwrite(occ_mask, mask)\n",
    "\n",
    "    return occ_img, occ_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "38fb7e57-f3f7-42f9-bca0-75c1761ef727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(126, 64)\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./occluders/fg_img_1879.jpg', './occluders/fg_mask_1879.png')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_occluder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3138f32b-47d2-44d2-9dff-51d5deb6463f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from libcom import color_transfer\n",
    "from libcom.utils.process_image import *\n",
    "from libcom.utils.environment import *\n",
    "from libcom import OPAScoreModel\n",
    "from libcom import get_composite_image\n",
    "from libcom.utils.process_image import make_image_grid\n",
    "import cv2\n",
    "import csv\n",
    "from PIL import Image\n",
    "from libcom.fopa_heat_map.source.prepare_multi_fg_scales import prepare_multi_fg_scales\n",
    "from libcom import Mure_ObjectStitchModel\n",
    "from libcom import ControlComModel\n",
    "\n",
    "def generate_composite_image(bg_img, bg_w, bg_h, bg_mask, bg_bbox):\n",
    "\n",
    "    net = Mure_ObjectStitchModel(device=0, sampler='plms')\n",
    "\n",
    "    # fg_mask is a path to an image here.... but bg_mask is a numpy array!\n",
    "    fg_img, fg_mask = get_occluder()\n",
    "    \n",
    "    score, fg_img, fg_mask, bbox, comp, comp_mask = get_optimal_location(fg_img, fg_mask, bg_img, bg_mask, bg_w, bg_h, bg_bbox, num_scales=16)\n",
    "    \n",
    "    if(score):\n",
    "        res, show_fg_img = net(bg_img, [fg_img], [fg_mask], bbox, sample_steps=25, num_samples=3)\n",
    "        bg_img   = draw_bbox_on_image(bg_img, bbox)\n",
    "        grid_img = make_image_grid([bg_img, fg_img, comp, comp_mask] + [res[i] for i in range(3)])\n",
    "        cv2.imwrite(f'./results/small_comp_{score}.jpg', grid_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6b12fc27-ed49-4054-b70e-f934f343fecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import BboxTools as bbt\n",
    "\n",
    "def generate_dataset(cate, file_list, bg_img_dir, bg_anno_dir, bg_mask_dir, record_file):\n",
    "    for file_name in file_list[:1]:\n",
    "        bg_w, bg_h, bg_bbox, flag_ = load_one_annotation(os.path.join(bg_anno_dir, file_name + '.mat'))\n",
    "\n",
    "        if flag_:\n",
    "            record_file.write('Skipped %s for multi objects\\n' % file_name)\n",
    "            continue\n",
    "\n",
    "        bg_img = os.path.join(bg_img_dir, file_name + '.JPEG')\n",
    "        bg_mask = np.array(Image.open(os.path.join(bg_mask_dir, file_name + '.JPEG')))\n",
    "\n",
    "        #if not bg_mask.shape[0] == bg_img.shape[0] and bg_mask.shape[1] == bg_img.shape[1]:\n",
    "            #bg_mask = cv2.resize(mask, (bg_img.shape[1], bg_img.shape[0]), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "        # not sure why this is necessary, took from OccludedPascal3D+ code\n",
    "        #bg_bbox = bbt.from_numpy(bg_bbox, image_boundary=img.shape[0:2], sorts=('x0', 'y0', 'x1', 'y1'))\n",
    "\n",
    "        generate_composite_image(bg_img, bg_w, bg_h, bg_mask, bg_bbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1a9a55e4-662a-4975-bd90-b7d955d4ccc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start cate:  aeroplane\n",
      "Start cate:  bicycle\n",
      "Start cate:  bus\n",
      "Start cate:  car\n",
      "Start cate:  motorbike\n",
      "Start cate:  train\n"
     ]
    }
   ],
   "source": [
    "categories = ['aeroplane', 'bicycle', 'bus', 'car', 'motorbike', 'train']\n",
    "\n",
    "for cate in categories:\n",
    "    print('Start cate: ', cate)\n",
    "    tem = open('generating_record_%s_1030.txt' % cate, 'w')\n",
    "    file_list_ = open(bg_list_path % cate).readlines()\n",
    "    file_list_ = [tem.strip('\\n') for tem in file_list_]\n",
    "    bg_img_path_ = bg_img_path % cate\n",
    "    bg_anno_path_ = bg_anno_path % cate\n",
    "    bg_mask_path_ = bg_mask_path % cate\n",
    "\n",
    "    generate_dataset(cate, file_list_, bg_img_path_, bg_anno_path_, bg_mask_path_, tem)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LibCom Environment",
   "language": "python",
   "name": "libcom"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
