{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "86330c65-d792-4845-91e5-d32d685f86b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "This is where everything ends up from the creation notebook\n",
    "'''\n",
    "\n",
    "path_saves = './realistic', './unrealistic'\n",
    "\n",
    "\n",
    "levels = ['low', 'medium', 'high']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d2085a19-e6bd-4497-80e6-82ac9b598e85",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize global storage\n",
    "global_storage = {\n",
    "    'occ_lvl': [],\n",
    "    'confidence': [],\n",
    "    'accuracy_iou': [],\n",
    "    'accuracy_label': [],\n",
    "    'average_precision': []\n",
    "}\n",
    "\n",
    "# Validate occlusion levels\n",
    "def validate_occlusion_level(occ_lvl):\n",
    "    valid_levels = {\"low\", \"medium\", \"high\"}\n",
    "    if occ_lvl.lower() not in valid_levels:\n",
    "        raise ValueError(f\"Invalid occlusion level '{occ_lvl}'. Expected one of {valid_levels}.\")\n",
    "    return occ_lvl.lower()\n",
    "\n",
    "# Example function to add data\n",
    "def store_metrics(occ_lvl, confidence, accuracy_iou, accuracy_label, average_precision):\n",
    "    \"\"\"\n",
    "    Stores the provided metrics in global storage after validation.\n",
    "    \n",
    "    Parameters:\n",
    "        occ_lvl (str): Occlusion level ('low', 'medium', 'high').\n",
    "        confidence (float): Confidence value.\n",
    "        accuracy_iou (float): Intersection over Union accuracy.\n",
    "        accuracy_label (float): Label truth accuracy.\n",
    "        average_precision (float): Average Precision metric.\n",
    "    \"\"\"\n",
    "    validated_level = validate_occlusion_level(occ_lvl)\n",
    "    global_storage['occ_lvl'].append(validated_level)\n",
    "    global_storage['confidence'].append(confidence)\n",
    "    global_storage['accuracy_iou'].append(accuracy_iou)\n",
    "    global_storage['accuracy_label'].append(accuracy_label)\n",
    "    global_storage['average_precision'].append(average_precision)\n",
    "\n",
    "# Function to aggregate metrics by occlusion level\n",
    "def aggregate_metrics():\n",
    "    levels = [\"low\", \"medium\", \"high\"]\n",
    "    aggregated = {level: {\"confidence\": [], \"accuracy_iou\": [], \"accuracy_label\": [], \"average_precision\": []} for level in levels}\n",
    "\n",
    "    for occ_lvl, conf, iou, label, ap in zip(global_storage['occ_lvl'], global_storage['confidence'], global_storage['accuracy_iou'], global_storage['accuracy_label'], global_storage['average_precision']):\n",
    "        aggregated[occ_lvl][\"confidence\"].append(conf)\n",
    "        aggregated[occ_lvl][\"accuracy_iou\"].append(iou)\n",
    "        aggregated[occ_lvl][\"accuracy_label\"].append(label)\n",
    "        aggregated[occ_lvl][\"average_precision\"].append(ap)\n",
    "    \n",
    "    # Calculate averages\n",
    "    averages = {\n",
    "        level: {\n",
    "            \"confidence\": np.mean(aggregated[level][\"confidence\"]) if aggregated[level][\"confidence\"] else 0,\n",
    "            \"accuracy_iou\": np.mean(aggregated[level][\"accuracy_iou\"]) if aggregated[level][\"accuracy_iou\"] else 0,\n",
    "            \"accuracy_label\": np.mean(aggregated[level][\"accuracy_label\"]) if aggregated[level][\"accuracy_label\"] else 0,\n",
    "            \"average_precision\": np.mean(aggregated[level][\"average_precision\"]) if aggregated[level][\"average_precision\"] else 0\n",
    "        }\n",
    "        for level in levels\n",
    "    }\n",
    "    return averages\n",
    "\n",
    "# Function to plot AP separately\n",
    "def plot_average_precision(realisticness):\n",
    "    averages = aggregate_metrics()\n",
    "    levels = [\"low\", \"medium\", \"high\"]\n",
    "\n",
    "    # Extract AP values for plotting\n",
    "    average_precision = [averages[level][\"average_precision\"] for level in levels]\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.bar(levels, average_precision, color='skyblue', alpha=0.7)\n",
    "\n",
    "    # Add labels, title, and grid\n",
    "    plt.xlabel(\"Occlusion Levels\")\n",
    "    plt.ylabel(\"Average Precision (AP)\")\n",
    "    plt.title(f\"Average Precision (AP) by Occlusion Levels for {realisticness} Dataset\")\n",
    "    plt.ylim(0, 1)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Function to plot other metrics\n",
    "def plot_metrics(realisticness):\n",
    "    averages = aggregate_metrics()\n",
    "    levels = [\"low\", \"medium\", \"high\"]\n",
    "\n",
    "    # Extract metrics for plotting\n",
    "    confidence = [averages[level][\"confidence\"] for level in levels]\n",
    "    accuracy_iou = [averages[level][\"accuracy_iou\"] for level in levels]\n",
    "    accuracy_label = [averages[level][\"accuracy_label\"] for level in levels]\n",
    "\n",
    "    x = np.arange(len(levels))  # the label locations\n",
    "    width = 0.25  # the width of the bars\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "\n",
    "    # Confidence Bar\n",
    "    plt.bar(x - width, confidence, width, label='Confidence')\n",
    "    # Accuracy IoU Bar\n",
    "    plt.bar(x, accuracy_iou, width, label='Accuracy (IoU)')\n",
    "    # Accuracy Label Bar\n",
    "    plt.bar(x + width, accuracy_label, width, label='Accuracy (Label Truth)')\n",
    "\n",
    "    # Add labels, title, and legend\n",
    "    plt.xlabel(\"Occlusion Levels\")\n",
    "    plt.ylabel(\"Metrics\")\n",
    "    plt.title(f\"Metrics by Occlusion Levels for {realisticness} Dataset\")\n",
    "    plt.xticks(x, levels)\n",
    "    plt.legend()\n",
    "    plt.ylim(0, 1)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# # Example of adding metrics\n",
    "# store_metrics(0.1, 0.85, 0.9, 0.88)\n",
    "# store_metrics(0.2, 0.8, 0.85, 0.86)\n",
    "\n",
    "# # Plot the graphs\n",
    "# plot_metrics()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "efa76bbb-5ba6-419f-885b-f8fc5fe4b46a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Intersection Over Union\n",
    "'''\n",
    "# To calculate IoU (intersection over union, given two bounding boxes)\n",
    "def bounding_box_intersection_over_union(box_predicted, box_truth):\n",
    "    # get (x, y) coordinates of intersection of bounding boxes\n",
    "    top_x_intersect = max(box_predicted[0], box_truth[0])\n",
    "    top_y_intersect = max(box_predicted[1], box_truth[1])\n",
    "    bottom_x_intersect = min(box_predicted[2], box_truth[2])\n",
    "    bottom_y_intersect = min(box_predicted[3], box_truth[3])\n",
    "\n",
    "    # calculate area of the intersection bb (bounding box)\n",
    "    intersection_area = max(0, bottom_x_intersect - top_x_intersect + 1) * max(\n",
    "        0, bottom_y_intersect - top_y_intersect + 1\n",
    "    )\n",
    "\n",
    "    # calculate area of the prediction bb and ground-truth bb\n",
    "    box_predicted_area = (box_predicted[2] - box_predicted[0] + 1) * (\n",
    "        box_predicted[3] - box_predicted[1] + 1\n",
    "    )\n",
    "    box_truth_area = (box_truth[2] - box_truth[0] + 1) * (\n",
    "        box_truth[3] - box_truth[1] + 1\n",
    "    )\n",
    "\n",
    "    # calculate intersection over union by taking intersection\n",
    "    # area and dividing it by the sum of predicted bb and ground truth\n",
    "    # bb areas subtracted by the intersection area\n",
    "    return intersection_area / float(\n",
    "        box_predicted_area + box_truth_area - intersection_area\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2acbb1b9-d7e1-46b3-b236-f07875107f0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def read_annotation(anno_file):\n",
    "    with open(anno_file, 'r') as f:\n",
    "        # Load the JSON data into a Python dictionary\n",
    "\n",
    "        anno = json.load(f)\n",
    "\n",
    "        bg_bbox = anno['box']\n",
    "        occlusion_ratio = anno['ratio']\n",
    "        occluder_bbox = anno['occluder_box']\n",
    "        occluder_path = anno['occluder_path']\n",
    "        bg_img = anno['source']\n",
    "        cate = anno['cate']\n",
    "\n",
    "    return bg_bbox, occlusion_ratio, occluder_bbox, occluder_path, bg_img, cate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3e642c32-f2eb-4458-9829-5442451c4bdd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_one_img(processor, model, img_path):\n",
    "    img = Image.open(img_path)\n",
    "    \n",
    "    inputs = processor(images=img, return_tensors=\"pt\")\n",
    "    outputs = model(**inputs)\n",
    "    \n",
    "    target_sizes = torch.tensor([img.size[::-1]])\n",
    "    results = processor.post_process_object_detection(outputs, target_sizes=target_sizes, threshold=0.9)[0]\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "72ef34e4-de52-499c-86b6-366eed4d494c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_updated_bb(bg_bbox, bg_w, bg_h, comp_w, comp_h):\n",
    "    scale_w = comp_w / bg_w\n",
    "    scale_h = comp_h / bg_h\n",
    "    \n",
    "    [x1, y1, x2, y2] = bg_bbox\n",
    "    \n",
    "    bb_w = x2 - x1\n",
    "    bb_h = y2 - y1\n",
    "    \n",
    "    x1_new = int(x1 * scale_w)\n",
    "    y1_new = int(y1 * scale_h)\n",
    "    x2_new = int(x2 * scale_w)\n",
    "    y2_new = int(y2 * scale_h)\n",
    "    \n",
    "    return [x1_new, y1_new, x2_new, y2_new]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784ee9d7-41b8-46c8-b67a-bf603b963fd1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Adapted for the Libcom output\n",
    "'''\n",
    "\n",
    "from transformers import DetrImageProcessor, DetrForObjectDetection\n",
    "import torch\n",
    "from PIL import Image\n",
    "import requests\n",
    "import os\n",
    "import cv2\n",
    "from libcom.utils.process_image import make_image_grid, draw_bbox_on_image\n",
    "import numpy as np\n",
    "\n",
    "# you can specify the revision tag if you don't want the timm dependency\n",
    "processor = DetrImageProcessor.from_pretrained(\"facebook/detr-resnet-50\", revision=\"no_timm\")\n",
    "model = DetrForObjectDetection.from_pretrained(\"facebook/detr-resnet-50\", revision=\"no_timm\")\n",
    " \n",
    "for path_save in path_saves:\n",
    "    save_img_path = path_save + '/images'\n",
    "    save_anno_path = path_save + '/annotations'\n",
    "    for level in levels:\n",
    "        print('------------------------')\n",
    "        print(f'Occlusion level: {level}')\n",
    "        print('------------------------')\n",
    "    \n",
    "        mean_iou = 0\n",
    "        mean_confidence = 0\n",
    "        correct_label_count = 0\n",
    "        total_predictions = 0\n",
    "        false_positives = 0\n",
    "        \n",
    "        for img_name in os.listdir(os.path.join(save_img_path, level)):\n",
    "            \n",
    "            if img_name[0] == '.':\n",
    "                continue\n",
    "        \n",
    "            img_id = img_name.split('.')[0]\n",
    "            img_path = os.path.join(save_img_path, level, img_name)\n",
    "        \n",
    "            results = process_one_img(processor, model, img_path)\n",
    "        \n",
    "            box_truth, occlusion_ratio, occluder_bbox, occluder_path, bg_img, label_truth = read_annotation(os.path.join(save_anno_path, level, img_id + '.json'))\n",
    "            \n",
    "            bg_img = Image.open(bg_img) \n",
    "            bg_w = bg_img.width \n",
    "            bg_h = bg_img.height \n",
    "            \n",
    "            comp_img = Image.open(img_path) \n",
    "            comp_w = comp_img.width \n",
    "            comp_h = comp_img.height \n",
    "            \n",
    "            box_truth_new = get_updated_bb(box_truth, bg_w, bg_h, comp_w, comp_h)\n",
    "            \n",
    "            # print(f\"Ground truth: {box_truth}\")\n",
    "        \n",
    "            greatest_iou = 0\n",
    "            greatest_iou_label = ''\n",
    "            total_predictions += 1\n",
    "            greatest_box_predicted = None\n",
    "            greatest_confidence = 0\n",
    "        \n",
    "            # Loops through every bounding box/annotation that the evaluation model produces, \n",
    "            for i, (score, label, box) in enumerate(zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"])):\n",
    "            \n",
    "                box_predicted = [round(i, 2) for i in box.tolist()]\n",
    "                label_predicted = model.config.id2label[label.item()]\n",
    "                confidence = round(score.item(), 3)\n",
    "        \n",
    "                box = [round(i, 2) for i in box.tolist()]\n",
    "                \n",
    "        \n",
    "                # Calculate IoU\n",
    "                iou = bounding_box_intersection_over_union(box_predicted, box_truth_new)\n",
    "                \n",
    "                if iou >= 0.5:\n",
    "                    if confidence > greatest_confidence:\n",
    "                        greatest_confidence = confidence\n",
    "                        if (label_predicted == \"motorcycle\" and label_truth == \"motorbike\") or (label_predicted == label_truth):\n",
    "                            greatest_iou = iou\n",
    "                            greatest_iou_label = label_predicted\n",
    "                            greatest_box_predicted = np.array(box_predicted).astype(int)\n",
    "                        else: \n",
    "                            false_positives += 1\n",
    "                else:\n",
    "                    if (label_predicted == \"motorcycle\" and label_truth == \"motorbike\") or (label_predicted == label_truth):\n",
    "                        false_positives += 1\n",
    "                        \n",
    "\n",
    "                # precision: (true positives)/(true positives + false positives) \n",
    "    \n",
    "                    \n",
    "            # Comparing predicted label to truth label  \n",
    "            if greatest_iou_label == label_truth:\n",
    "                # Tracking this label's bounding box \n",
    "                correct_label_count += 1\n",
    "\n",
    "    \n",
    "            mean_iou += greatest_iou\n",
    "            mean_confidence += greatest_confidence\n",
    "            \n",
    "            # cv2.imwrite(f'./predictions/{img_id}.jpg', grid_img)\n",
    "     \n",
    "            # # Final statistics\n",
    "        \n",
    "        print(f\"Mean IoU: {mean_iou / total_predictions:.3f}\")\n",
    "        print(f\"Total Correct Labels: {correct_label_count}\")\n",
    "        print(f\"Avg Correct Labels: {correct_label_count / total_predictions:.3f}\\n\")\n",
    "        print(f\"False Positives: {false_positives}\")\n",
    "        print(f\"AP: {correct_label_count / (correct_label_count + false_positives)}\")\n",
    "        store_metrics(level ,(mean_confidence/ total_predictions), (mean_iou / total_predictions), (correct_label_count / total_predictions), (correct_label_count / (correct_label_count + false_positives)))\n",
    "        \n",
    "    if path_save == './realistic':\n",
    "        plot_metrics(\"Realistic\")\n",
    "        plot_average_precision(\"Realistic\")\n",
    "    elif path_save == './unrealistic':\n",
    "        plot_metrics(\"Unrealistic\")\n",
    "        plot_average_precision(\"Unrealistic\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "babbfb65-c46d-4ec1-9e3f-d55028821a5a",
   "metadata": {},
   "source": [
    "'''\n",
    "Original Code\n",
    "\n",
    "depreciated, don't run this. \n",
    "'''\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "# To calculate IoU (intersection over union, given two bounding boxes)\n",
    "def bounding_box_intersection_over_union(box_predicted, box_truth):\n",
    "    # get (x, y) coordinates of intersection of bounding boxes\n",
    "    top_x_intersect = max(box_predicted[0], box_truth[0])\n",
    "    top_y_intersect = max(box_predicted[1], box_truth[1])\n",
    "    bottom_x_intersect = min(box_predicted[2], box_truth[2])\n",
    "    bottom_y_intersect = min(box_predicted[3], box_truth[3])\n",
    "\n",
    "    # calculate area of the intersection bb (bounding box)\n",
    "    intersection_area = max(0, bottom_x_intersect - top_x_intersect + 1) * max(\n",
    "        0, bottom_y_intersect - top_y_intersect + 1\n",
    "    )\n",
    "\n",
    "    # calculate area of the prediction bb and ground-truth bb\n",
    "    box_predicted_area = (box_predicted[2] - box_predicted[0] + 1) * (\n",
    "        box_predicted[3] - box_predicted[1] + 1\n",
    "    )\n",
    "    box_truth_area = (box_truth[2] - box_truth[0] + 1) * (\n",
    "        box_truth[3] - box_truth[1] + 1\n",
    "    )\n",
    "\n",
    "    # calculate intersection over union by taking intersection\n",
    "    # area and dividing it by the sum of predicted bb and ground truth\n",
    "    # bb areas subtracted by the intersection area\n",
    "    return intersection_area / float(\n",
    "        box_predicted_area + box_truth_area - intersection_area\n",
    "    )\n",
    "\n",
    "\n",
    "# Ground truth data for comparison (dummy values for demonstration)\n",
    "ground_truth_boxes = [\n",
    "    [120, 80, 300, 250],  # Example ground truth bounding boxes\n",
    "]\n",
    "ground_truth_labels = [1]  # Example ground truth labels (change as per your dataset)\n",
    "\n",
    "mean_iou = 0\n",
    "correct_label_count = 0\n",
    "total_predictions = 0\n",
    "\n",
    "# Process and compare the results\n",
    "for i, (score, label, box) in enumerate(zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"])):\n",
    "    total_predictions += 1\n",
    "\n",
    "    # Predicted bounding box and label\n",
    "    box_predicted = [round(i, 2) for i in box.tolist()]\n",
    "    label_predicted = label.item()\n",
    "\n",
    "    # Ground truth bounding box and label for comparison\n",
    "    box_truth = ground_truth_boxes[i]\n",
    "    label_truth = ground_truth_labels[i]\n",
    "\n",
    "    # Calculate IoU\n",
    "    iou = bounding_box_intersection_over_union(box_predicted, box_truth)\n",
    "    mean_iou += iou\n",
    "\n",
    "    # Check label correctness\n",
    "    if label_predicted == label_truth:\n",
    "        correct_label_count += 1\n",
    "\n",
    "    # Plot the results\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
    "    ax.imshow(image)\n",
    "    rect_pred = patches.Rectangle(\n",
    "        (box_predicted[0], box_predicted[1]),\n",
    "        box_predicted[2] - box_predicted[0],\n",
    "        box_predicted[3] - box_predicted[1],\n",
    "        linewidth=2, edgecolor='red', facecolor='none', label=\"Predicted\"\n",
    "    )\n",
    "    rect_truth = patches.Rectangle(\n",
    "        (box_truth[0], box_truth[1]),\n",
    "        box_truth[2] - box_truth[0],\n",
    "        box_truth[3] - box_truth[1],\n",
    "        linewidth=2, edgecolor='green', facecolor='none', linestyle='dashed', label=\"Ground Truth\"\n",
    "    )\n",
    "    ax.add_patch(rect_pred)\n",
    "    ax.add_patch(rect_truth)\n",
    "    plt.legend()\n",
    "    plt.title(f\"IoU: {iou:.3f}, Label Correct: {label_predicted == label_truth}\")\n",
    "    plt.show()\n",
    "\n",
    "# Final statistics\n",
    "print(f\"Mean IoU: {mean_iou / total_predictions:.3f}\")\n",
    "print(f\"Correct Labels: {correct_label_count}/{total_predictions}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69fc34a-8efa-49ae-bcf4-adf984575070",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
