{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9ec652f-1cbc-4cc0-a78d-0ee3af39d9e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openimages\n",
      "  Downloading openimages-0.0.1-py2.py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: tqdm in /opt/conda/envs/Libcom/lib/python3.8/site-packages (from openimages) (4.66.6)\n",
      "Requirement already satisfied: requests in /opt/conda/envs/Libcom/lib/python3.8/site-packages (from openimages) (2.32.3)\n",
      "Requirement already satisfied: boto3 in /opt/conda/envs/Libcom/lib/python3.8/site-packages (from openimages) (1.35.52)\n",
      "Requirement already satisfied: pandas in /opt/conda/envs/Libcom/lib/python3.8/site-packages (from openimages) (1.5.3)\n",
      "Requirement already satisfied: botocore<1.36.0,>=1.35.52 in /opt/conda/envs/Libcom/lib/python3.8/site-packages (from boto3->openimages) (1.35.52)\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /opt/conda/envs/Libcom/lib/python3.8/site-packages (from boto3->openimages) (0.10.3)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/envs/Libcom/lib/python3.8/site-packages (from boto3->openimages) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/envs/Libcom/lib/python3.8/site-packages (from botocore<1.36.0,>=1.35.52->boto3->openimages) (2.8.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/envs/Libcom/lib/python3.8/site-packages (from botocore<1.36.0,>=1.35.52->boto3->openimages) (1.26.14)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/envs/Libcom/lib/python3.8/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.36.0,>=1.35.52->boto3->openimages) (1.16.0)\n",
      "Collecting cvdata\n",
      "  Downloading cvdata-0.0.3-py3-none-any.whl (37 kB)\n",
      "Requirement already satisfied: opencv-python in /opt/conda/envs/Libcom/lib/python3.8/site-packages (from cvdata->openimages) (4.1.2.30)\n",
      "Requirement already satisfied: pillow in /opt/conda/envs/Libcom/lib/python3.8/site-packages (from cvdata->openimages) (10.1.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/envs/Libcom/lib/python3.8/site-packages (from cvdata->openimages) (1.22.3)\n",
      "Collecting lxml\n",
      "  Downloading lxml-5.3.0-cp38-cp38-manylinux_2_28_x86_64.whl (5.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.1 MB 7.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pytz>=2020.1 in /opt/conda/envs/Libcom/lib/python3.8/site-packages (from pandas->openimages) (2024.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/Libcom/lib/python3.8/site-packages (from requests->openimages) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/Libcom/lib/python3.8/site-packages (from requests->openimages) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/Libcom/lib/python3.8/site-packages (from requests->openimages) (2023.7.22)\n",
      "Installing collected packages: lxml, cvdata, openimages\n",
      "Successfully installed cvdata-0.0.3 lxml-5.3.0 openimages-0.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install openimages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dcb189bd-f9b7-4d25-ad85-ba7a24990efa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-06  23:08:03 INFO Downloading 10 train images for class 'bicycle'\n",
      "100%|███████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 20.59it/s]\n",
      "2024-11-06  23:08:04 INFO Creating 10 train annotations (pascal) for class 'bicycle'\n",
      "100%|█████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 1357.03it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'bicycle': {'images_dir': './dest/dir/bicycle/images',\n",
       "  'annotations_dir': './dest/dir/bicycle/pascal'}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openimages.download import download_dataset\n",
    "download_dataset(\"./dest/dir\", [\"Bicycle\",], annotation_format=\"pascal\", limit=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd115461-ccab-4cff-bb72-4ed370b0ca68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "from pathlib import Path\n",
    "\n",
    "# Define the path where your annotations are stored\n",
    "annotations_path = Path(\"./dest/dir/car/pascal\")\n",
    "images_path = Path(\"./dest/dir/car/images\")\n",
    "\n",
    "# Function to parse the XML and get bounding boxes\n",
    "def parse_anno(annotation_file):\n",
    "    tree = ET.parse(annotation_file)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    bboxes = []\n",
    "    for obj in root.findall(\"object\"):\n",
    "        class_name = obj.find(\"name\").text\n",
    "        bbox = obj.find(\"bndbox\")\n",
    "        width = int(root.find(\"size/width\").text)\n",
    "        height = int(root.find(\"size/height\").text)\n",
    "        x_min = int(bbox.find(\"xmin\").text)\n",
    "        y_min = int(bbox.find(\"ymin\").text)\n",
    "        x_max = int(bbox.find(\"xmax\").text)\n",
    "        y_max = int(bbox.find(\"ymax\").text)\n",
    "        bboxes.append([x_min, y_min, x_max, y_max])\n",
    "\n",
    "    return width, height, max(bboxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "53198e4d-0862-4f71-b590-22a2e71ec3b2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m#test_set = get_test_list_fopa_heatmap()\u001b[39;00m\n\u001b[1;32m     15\u001b[0m test_set \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdataset\u001b[49m[\u001b[38;5;241m5\u001b[39m:\u001b[38;5;241m6\u001b[39m]:\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28mprint\u001b[39m(sample\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mwidth)\n\u001b[1;32m     18\u001b[0m     test_set\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mforeground\u001b[39m\u001b[38;5;124m'\u001b[39m: fg_img, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mforeground_mask\u001b[39m\u001b[38;5;124m'\u001b[39m: fg_mask, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbackground\u001b[39m\u001b[38;5;124m'\u001b[39m: sample\u001b[38;5;241m.\u001b[39mfilepath})\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Just testing the FOPA code\n",
    "'''\n",
    "\n",
    "from libcom import FOPAHeatMapModel\n",
    "from libcom.utils.process_image import make_image_grid, draw_bbox_on_image\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "fg_img = '/home/lradovan/workspace/libcom/tests/source/foreground/1.jpg'\n",
    "fg_mask = '/home/lradovan/workspace/libcom/tests/source/foreground_mask/1.png'\n",
    "\n",
    "#test_set = get_test_list_fopa_heatmap()\n",
    "\n",
    "test_set = []\n",
    "for sample in dataset[5:6]:\n",
    "    print(sample.metadata.width)\n",
    "    test_set.append({'foreground': fg_img, 'foreground_mask': fg_mask, 'background': sample.filepath})\n",
    "    \n",
    "result_dir = './results/'\n",
    "net = FOPAHeatMapModel(device=0)\n",
    "\n",
    "for pair in test_set[:1]:\n",
    "    fg_img, fg_mask, bg_img = pair['foreground'], pair['foreground_mask'], pair['background']\n",
    "    bboxes, heatmaps = net(fg_img, fg_mask, bg_img, cache_dir=os.path.join(result_dir, 'cache'), heatmap_dir=os.path.join(result_dir, 'heatmap'))\n",
    "    img_name  = os.path.basename(bg_img).replace('.png', '.jpg')\n",
    "    bbox = bboxes[0]\n",
    "    bg_img  = draw_bbox_on_image(bg_img, [bbox[0], bbox[1], bbox[0] + bbox[2], bbox[1] + bbox[3]])\n",
    "    grid_img  = make_image_grid([bg_img, fg_img, heatmaps[0]])\n",
    "    os.makedirs(os.path.join(result_dir, 'grid'), exist_ok=True)\n",
    "    res_path  = os.path.join(result_dir, 'grid', img_name)\n",
    "    cv2.imwrite(res_path, grid_img)\n",
    "    print('save result to ', res_path)\n",
    "print(f'end testing!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "62859c25-6ef3-4d75-84fb-d24ba8627537",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "get overlap percentage\n",
    "'''\n",
    "def overlap_ratio(occluder_bb, occludee_bb):\n",
    "\n",
    "    #top left and bottom right points\n",
    "    occluder_x1, occluder_y1, occluder_x2, occluder_y2 = occluder_bb\n",
    "    occludee_x1, occludee_y1, occludee_x2, occludee_y2 = occludee_bb\n",
    "\n",
    "    # area of the foreground object\n",
    "    occludee_area = (occludee_x2 - occludee_x1) * (occludee_y2 - occludee_y1)\n",
    "\n",
    "    # area of the background object being covered by the foreground object\n",
    "    overlap_area = max(0, min(occludee_x2, occluder_x2) - max(occludee_x1, occluder_x1)) * max(0, min(occludee_y2, occluder_y2) - max(occludee_y1, occluder_y1))\n",
    "\n",
    "    # overlap over the total background object area\n",
    "    return overlap_area / occludee_area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e40a3046-ebed-4deb-b3ce-5d75013792ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "'''\n",
    "Get a list of randomly chosen bounding boxes to occlude the background object above some threshold\n",
    "This can be improved if we know the foreground image has to be some base scale to allow for above threshold occlusion\n",
    "'''\n",
    "def get_bbox_list(bg_bbox, bg_w, bg_h, fg_w, fg_h):\n",
    "\n",
    "    bboxes = []\n",
    "\n",
    "    occludee_x1 = bg_bbox[0]\n",
    "    occludee_y1 = bg_bbox[1] \n",
    "    occludee_x2 = bg_bbox[2]\n",
    "    occludee_y2 = bg_bbox[3]\n",
    "\n",
    "    num_boxes = 5 # iterate over multiple boxes\n",
    "    overlap_threshold = .2 # ensure a minimum amount of overlap\n",
    "\n",
    "    # print(bg_bbox, fg_w, fg_h)\n",
    "    \n",
    "    for _ in range(num_boxes):\n",
    "        # top left corner of the occluder bounding box:\n",
    "\n",
    "        occluder_x1 = random.randint(max(0, occludee_x1 - fg_w), occludee_x2) # overlapping in the x-direction\n",
    "        occluder_y1 = random.randint(max(0, occludee_y1 - fg_h), occludee_y2) # overlapping in the y-direction\n",
    "\n",
    "        # TODO: Fix so doesnt exceed background image\n",
    "        \n",
    "        occluder_x2 = occluder_x1 + fg_w\n",
    "        occluder_y2 = occluder_y1 + fg_h\n",
    "\n",
    "        # Maybe this will fix sizing error??? (IT DOES)\n",
    "        if occluder_x2 > bg_w or occluder_y2 > bg_h:\n",
    "            continue\n",
    "\n",
    "        occluder_bb = [occluder_x1, occluder_y1, occluder_x2, occluder_y2]\n",
    "        occluded_ratio = overlap_ratio(occluder_bb, bg_bbox)\n",
    "\n",
    "        if occluded_ratio >= overlap_threshold:\n",
    "            bboxes.append([occluder_x1, occluder_y1, occluder_x2, occluder_y2])\n",
    "    \n",
    "    return bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0bcddcea-517d-4f4c-ae09-73ace7e0a378",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "returns the score, composite image, and compositive mask. \n",
    "num scales is the number of different foreground scales to try.\n",
    "'''\n",
    "from libcom import OPAScoreModel\n",
    "\n",
    "def get_optimal_location(fg_img, fg_mask, bg_img, bg_w, bg_h, bg_bbox, num_scales):\n",
    "\n",
    "    net = OPAScoreModel(device=0, model_type='SimOPA')\n",
    "    cache_dir = './cache'\n",
    "\n",
    "    # from libcom.fopa_heat_map.source.prepare_multi_fg_scales import prepare_multi_fg_scales\n",
    "    scaled_fg_dir, scaled_mask_dir, csv_path = prepare_multi_fg_scales(cache_dir, fg_img, fg_mask, bg_img, num_scales)\n",
    "\n",
    "    score = 0\n",
    "    optimal_bbox = None\n",
    "    best_fg = None\n",
    "    best_mask = None\n",
    "    \n",
    "    # iterate over the different foreground scales\n",
    "    with open(csv_path, mode='r', newline='') as csv_file:\n",
    "        csv_reader = csv.DictReader(csv_file)\n",
    "        for row in csv_reader:\n",
    "            fg_name   = '{}_{}_{}_{}.jpg'.format(row[\"fg_name\"].split(\".\")[0],row[\"bg_name\"].split(\".\")[0],int(row[\"newWidth\"]),int(row[\"newHeight\"]))\n",
    "            mask_name = '{}_{}_{}_{}.jpg'.format(row[\"fg_name\"].split(\".\")[0],row[\"bg_name\"].split(\".\")[0],int(row[\"newWidth\"]),int(row[\"newHeight\"]))\n",
    "            scale     = row['scale']\n",
    "            fg_w = int(row['newWidth'])\n",
    "            fg_h = int(row['newHeight'])\n",
    "            \n",
    "            save_name = fg_name.split(\".\")[0] + '_' + str(scale) + '.jpg'\n",
    "            bg_img    = read_image_pil(bg_img)\n",
    "            fg_img    = read_image_pil(os.path.join(scaled_fg_dir, fg_name))\n",
    "            fg_mask   = read_mask_pil(os.path.join(scaled_mask_dir, mask_name))\n",
    "            bbox_list = get_bbox_list(bg_bbox, bg_w, bg_h, fg_w, fg_h)\n",
    "    \n",
    "            for bbox in bbox_list:\n",
    "                comp, comp_mask = get_composite_image(fg_img, fg_mask, bg_img, bbox)\n",
    "                bbox_score = net(comp, comp_mask)\n",
    "                if bbox_score > score:\n",
    "                    best_fg = os.path.join(scaled_fg_dir, fg_name)\n",
    "                    best_mask = os.path.join(scaled_mask_dir, mask_name)\n",
    "                    optimal_bbox = bbox\n",
    "                    score = bbox_score\n",
    "\n",
    "        return score, best_fg, best_mask, optimal_bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "72bb5c37-f15f-4698-a0aa-7183cc4f0210",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 2.00 GiB (GPU 0; 21.95 GiB total capacity; 10.71 GiB already allocated; 1.87 GiB free; 18.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 42\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m(score):\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;66;03m# print(fg_img, fg_mask)\u001b[39;00m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;66;03m# grid_img  = make_image_grid([comp, comp_mask])\u001b[39;00m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;66;03m# cv2.imwrite(f'./results/composite_{image_id}.jpg', grid_img)\u001b[39;00m\n\u001b[1;32m     41\u001b[0m     net      \u001b[38;5;241m=\u001b[39m ControlComModel(device\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, model_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mControlCom\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 42\u001b[0m     comp     \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbg_img\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfg_img\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbbox\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfg_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mblending\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mharmonization\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m     bg_img   \u001b[38;5;241m=\u001b[39m draw_bbox_on_image(bg_img, bbox)\n\u001b[1;32m     44\u001b[0m     grid_img \u001b[38;5;241m=\u001b[39m make_image_grid([bg_img, fg_img, comp[\u001b[38;5;241m0\u001b[39m], comp[\u001b[38;5;241m1\u001b[39m]])\n",
      "File \u001b[0;32m/opt/conda/envs/Libcom/lib/python3.8/site-packages/torch/autograd/grad_mode.py:28\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m():\n\u001b[0;32m---> 28\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/Libcom/lib/python3.8/site-packages/libcom/controllable_composition/controllable_composition.py:219\u001b[0m, in \u001b[0;36mControlComModel.__call__\u001b[0;34m(self, background_image, foreground_image, bbox, foreground_mask, task, num_samples, sample_steps, guidance_scale, seed)\u001b[0m\n\u001b[1;32m    217\u001b[0m start_code \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn([num_samples]\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlatent_shape, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    218\u001b[0m start_code \u001b[38;5;241m=\u001b[39m start_code\u001b[38;5;241m.\u001b[39mrepeat(\u001b[38;5;28mlen\u001b[39m(indicator), \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 219\u001b[0m outputs, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msampler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mS\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mconditioning\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mindicator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlatent_shape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m                            \u001b[49m\u001b[43meta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mx_T\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m                            \u001b[49m\u001b[43munconditional_guidance_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mguidance_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m                            \u001b[49m\u001b[43munconditional_conditioning\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mtest_model_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    229\u001b[0m comp_img   \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutputs_postprocess(outputs)\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m comp_img\n",
      "File \u001b[0;32m/opt/conda/envs/Libcom/lib/python3.8/site-packages/torch/autograd/grad_mode.py:28\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m():\n\u001b[0;32m---> 28\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/Libcom/lib/python3.8/site-packages/libcom/controllable_composition/source/ControlCom/ldm/models/diffusion/ddim.py:98\u001b[0m, in \u001b[0;36mDDIMSampler.sample\u001b[0;34m(self, S, batch_size, shape, conditioning, callback, normals_sequence, img_callback, quantize_x0, eta, mask, x0, temperature, noise_dropout, score_corrector, corrector_kwargs, verbose, x_T, log_every_t, unconditional_guidance_scale, unconditional_conditioning, **kwargs)\u001b[0m\n\u001b[1;32m     93\u001b[0m size \u001b[38;5;241m=\u001b[39m (batch_size, C, H, W)\n\u001b[1;32m     94\u001b[0m \u001b[38;5;66;03m# print(f'Data shape for DDIM sampling is {size}, eta {eta}')\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;66;03m# if unconditional_guidance_scale > 1 and unconditional_conditioning != None: \u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m#     print('using classifier-free guidance for sampling')\u001b[39;00m\n\u001b[0;32m---> 98\u001b[0m samples, intermediates \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mddim_sampling\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconditioning\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mimg_callback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimg_callback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mquantize_denoised\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquantize_x0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mddim_use_original_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mnoise_dropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnoise_dropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mscore_corrector\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscore_corrector\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mcorrector_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcorrector_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mx_T\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx_T\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mlog_every_t\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_every_t\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43munconditional_guidance_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munconditional_guidance_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43munconditional_conditioning\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munconditional_conditioning\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m                                            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m samples, intermediates\n",
      "File \u001b[0;32m/opt/conda/envs/Libcom/lib/python3.8/site-packages/torch/autograd/grad_mode.py:28\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m():\n\u001b[0;32m---> 28\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/Libcom/lib/python3.8/site-packages/libcom/controllable_composition/source/ControlCom/ldm/models/diffusion/ddim.py:159\u001b[0m, in \u001b[0;36mDDIMSampler.ddim_sampling\u001b[0;34m(self, cond, shape, x_T, ddim_use_original_steps, callback, timesteps, quantize_denoised, mask, x0, img_callback, log_every_t, temperature, noise_dropout, score_corrector, corrector_kwargs, unconditional_guidance_scale, unconditional_conditioning, **kwargs)\u001b[0m\n\u001b[1;32m    156\u001b[0m     img_orig \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mq_sample(inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbg_latent\u001b[39m\u001b[38;5;124m'\u001b[39m], ts)  \u001b[38;5;66;03m# TODO: deterministic forward pass?\u001b[39;00m\n\u001b[1;32m    157\u001b[0m     img \u001b[38;5;241m=\u001b[39m img_orig \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m mask) \u001b[38;5;241m+\u001b[39m mask \u001b[38;5;241m*\u001b[39m img\n\u001b[0;32m--> 159\u001b[0m outs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp_sample_ddim\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcond\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_original_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mddim_use_original_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mquantize_denoised\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquantize_denoised\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mnoise_dropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnoise_dropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscore_corrector\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscore_corrector\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mcorrector_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcorrector_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m                          \u001b[49m\u001b[43munconditional_guidance_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munconditional_guidance_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m                          \u001b[49m\u001b[43munconditional_conditioning\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munconditional_conditioning\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;66;03m# img, pred_mask, pred_x0 = outs\u001b[39;00m\n\u001b[1;32m    166\u001b[0m img, pred_x0, attn_t \u001b[38;5;241m=\u001b[39m outs\n",
      "File \u001b[0;32m/opt/conda/envs/Libcom/lib/python3.8/site-packages/torch/autograd/grad_mode.py:28\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m():\n\u001b[0;32m---> 28\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/Libcom/lib/python3.8/site-packages/libcom/controllable_composition/source/ControlCom/ldm/models/diffusion/ddim.py:205\u001b[0m, in \u001b[0;36mDDIMSampler.p_sample_ddim\u001b[0;34m(self, x, c, t, index, mask, repeat_noise, use_original_steps, quantize_denoised, temperature, noise_dropout, score_corrector, corrector_kwargs, unconditional_guidance_scale, unconditional_conditioning, **kwargs)\u001b[0m\n\u001b[1;32m    203\u001b[0m t_in \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([t] \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    204\u001b[0m c_in \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39mcat([uc_item, c_item]) \u001b[38;5;28;01mfor\u001b[39;00m uc_item,c_item \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(unconditional_conditioning, c)]\n\u001b[0;32m--> 205\u001b[0m e_t_double, attn_t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_in\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbbox_in\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_in\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc_in\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m e_t_uncond, e_t \u001b[38;5;241m=\u001b[39m e_t_double\u001b[38;5;241m.\u001b[39mchunk(\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    207\u001b[0m e_t \u001b[38;5;241m=\u001b[39m e_t_uncond \u001b[38;5;241m+\u001b[39m unconditional_guidance_scale \u001b[38;5;241m*\u001b[39m (e_t \u001b[38;5;241m-\u001b[39m e_t_uncond)\n",
      "File \u001b[0;32m/opt/conda/envs/Libcom/lib/python3.8/site-packages/libcom/controllable_composition/source/ControlCom/ldm/models/diffusion/ddpm.py:962\u001b[0m, in \u001b[0;36mLatentDiffusion.apply_model\u001b[0;34m(self, x_noisy, bbox, t, cond, **kwargs)\u001b[0m\n\u001b[1;32m    960\u001b[0m     cond \u001b[38;5;241m=\u001b[39m {key: cond}\n\u001b[1;32m    961\u001b[0m x \u001b[38;5;241m=\u001b[39m [x_noisy, bbox]\n\u001b[0;32m--> 962\u001b[0m x_recon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcond\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    963\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x_recon\n",
      "File \u001b[0;32m/opt/conda/envs/Libcom/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/envs/Libcom/lib/python3.8/site-packages/libcom/controllable_composition/source/ControlCom/ldm/models/diffusion/ddpm.py:1547\u001b[0m, in \u001b[0;36mDiffusionWrapper.forward\u001b[0;34m(self, x, t, c_concat, c_crossattn, **kwargs)\u001b[0m\n\u001b[1;32m   1544\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconditioning_key \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcrossattn\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m   1545\u001b[0m     \u001b[38;5;66;03m# cc = torch.cat(c_crossattn, 1)\u001b[39;00m\n\u001b[1;32m   1546\u001b[0m     cc  \u001b[38;5;241m=\u001b[39m c_crossattn\n\u001b[0;32m-> 1547\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdiffusion_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1548\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconditioning_key \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhybrid\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m   1549\u001b[0m     xc \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([x] \u001b[38;5;241m+\u001b[39m c_concat, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/envs/Libcom/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/envs/Libcom/lib/python3.8/site-packages/libcom/controllable_composition/source/ControlCom/ldm/modules/diffusionmodules/openaimodel.py:998\u001b[0m, in \u001b[0;36mUNetModel.forward\u001b[0;34m(self, x_bbox, timesteps, context, y, **kwargs)\u001b[0m\n\u001b[1;32m    996\u001b[0m         h \u001b[38;5;241m=\u001b[39m module(h, emb, context, local_context, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    997\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_blocks:\n\u001b[0;32m--> 998\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43memb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_context\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    999\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m   1000\u001b[0m         h,attn \u001b[38;5;241m=\u001b[39m out\n",
      "File \u001b[0;32m/opt/conda/envs/Libcom/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/envs/Libcom/lib/python3.8/site-packages/libcom/controllable_composition/source/ControlCom/ldm/modules/diffusionmodules/openaimodel.py:103\u001b[0m, in \u001b[0;36mTimestepEmbedSequential.forward\u001b[0;34m(self, x, emb, context, local_context, **kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m     x \u001b[38;5;241m=\u001b[39m layer(x, emb)\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(layer, SpatialTransformer):\n\u001b[0;32m--> 103\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(layer, (LocalRefineBlock,)):\n\u001b[1;32m    105\u001b[0m     out \u001b[38;5;241m=\u001b[39m layer(x, local_context, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/conda/envs/Libcom/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/envs/Libcom/lib/python3.8/site-packages/libcom/controllable_composition/source/ControlCom/ldm/modules/attention.py:261\u001b[0m, in \u001b[0;36mSpatialTransformer.forward\u001b[0;34m(self, x, context)\u001b[0m\n\u001b[1;32m    259\u001b[0m x \u001b[38;5;241m=\u001b[39m rearrange(x, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb c h w -> b (h w) c\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer_blocks:\n\u001b[0;32m--> 261\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    262\u001b[0m x \u001b[38;5;241m=\u001b[39m rearrange(x, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb (h w) c -> b c h w\u001b[39m\u001b[38;5;124m'\u001b[39m, h\u001b[38;5;241m=\u001b[39mh, w\u001b[38;5;241m=\u001b[39mw)\n\u001b[1;32m    263\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproj_out(x)\n",
      "File \u001b[0;32m/opt/conda/envs/Libcom/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/envs/Libcom/lib/python3.8/site-packages/libcom/controllable_composition/source/ControlCom/ldm/modules/attention.py:212\u001b[0m, in \u001b[0;36mBasicTransformerBlock.forward\u001b[0;34m(self, x, context)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, context\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 212\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcheckpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheckpoint\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/Libcom/lib/python3.8/site-packages/libcom/controllable_composition/source/ControlCom/ldm/modules/diffusionmodules/util.py:114\u001b[0m, in \u001b[0;36mcheckpoint\u001b[0;34m(func, inputs, params, flag)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m flag:\n\u001b[1;32m    113\u001b[0m     args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(inputs) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mtuple\u001b[39m(params)\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mCheckpointFunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39minputs)\n",
      "File \u001b[0;32m/opt/conda/envs/Libcom/lib/python3.8/site-packages/libcom/controllable_composition/source/ControlCom/ldm/modules/diffusionmodules/util.py:127\u001b[0m, in \u001b[0;36mCheckpointFunction.forward\u001b[0;34m(ctx, run_function, length, *args)\u001b[0m\n\u001b[1;32m    124\u001b[0m ctx\u001b[38;5;241m.\u001b[39minput_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(args[length:])\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 127\u001b[0m     output_tensors \u001b[38;5;241m=\u001b[39m \u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output_tensors\n",
      "File \u001b[0;32m/opt/conda/envs/Libcom/lib/python3.8/site-packages/libcom/controllable_composition/source/ControlCom/ldm/modules/attention.py:215\u001b[0m, in \u001b[0;36mBasicTransformerBlock._forward\u001b[0;34m(self, x, context)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_forward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, context\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 215\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn1\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m x\n\u001b[1;32m    216\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x), context\u001b[38;5;241m=\u001b[39mcontext) \u001b[38;5;241m+\u001b[39m x\n\u001b[1;32m    217\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mff(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm3(x)) \u001b[38;5;241m+\u001b[39m x\n",
      "File \u001b[0;32m/opt/conda/envs/Libcom/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/envs/Libcom/lib/python3.8/site-packages/libcom/controllable_composition/source/ControlCom/ldm/modules/attention.py:183\u001b[0m, in \u001b[0;36mCrossAttention.forward\u001b[0;34m(self, x, context, mask)\u001b[0m\n\u001b[1;32m    179\u001b[0m v \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_v(context)\n\u001b[1;32m    181\u001b[0m q, k, v \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m t: rearrange(t, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb n (h d) -> (b h) n d\u001b[39m\u001b[38;5;124m'\u001b[39m, h\u001b[38;5;241m=\u001b[39mh), (q, k, v))\n\u001b[0;32m--> 183\u001b[0m sim \u001b[38;5;241m=\u001b[39m \u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mb i d, b j d -> b i j\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exists(mask):\n\u001b[1;32m    186\u001b[0m     mask \u001b[38;5;241m=\u001b[39m rearrange(mask, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb ... -> b (...)\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 2.00 GiB (GPU 0; 21.95 GiB total capacity; 10.71 GiB already allocated; 1.87 GiB free; 18.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Right now this just takes does one specific foreground image (teddy bear), and one background image (skier)\n",
    "'''\n",
    "\n",
    "from libcom.utils.process_image import *\n",
    "from libcom.utils.environment import *\n",
    "from libcom import OPAScoreModel\n",
    "from libcom import get_composite_image\n",
    "from libcom.utils.process_image import make_image_grid\n",
    "import cv2\n",
    "import csv\n",
    "from PIL import Image\n",
    "from libcom.fopa_heat_map.source.prepare_multi_fg_scales import prepare_multi_fg_scales\n",
    "\n",
    "from libcom import ControlComModel\n",
    "\n",
    "net = OPAScoreModel(device=0, model_type='SimOPA')\n",
    "\n",
    "# for each scale, take the highest OPA score\n",
    "fg_scale_num = 16\n",
    "\n",
    "# trying with just one foreground image (teddy bear) and one background image (skier)\n",
    "foreground_image = '../workspace/libcom/tests/source/foreground/1.jpg'\n",
    "foreground_mask = '../workspace/libcom/tests/source/foreground_mask/1.png'\n",
    "\n",
    "# Define the path where your annotations are stored\n",
    "bg_annotations_path = Path(\"./dest/dir/car/pascal\")\n",
    "bg_images_path = Path(\"./dest/dir/car/images\")\n",
    "\n",
    "for annotation_file in annotations_path.glob(\"*.xml\"):\n",
    "    image_id = annotation_file.stem\n",
    "    bg_img = os.path.join(bg_images_path, f\"{image_id}.jpg\")\n",
    "    bg_w, bg_h, bg_bbox = parse_anno(annotation_file)\n",
    "    score, fg_img, fg_mask, bbox = get_optimal_location(foreground_image, foreground_mask, bg_img, bg_w, bg_h, bg_bbox, num_scales=16)\n",
    "    \n",
    "    if(score):\n",
    "        # print(fg_img, fg_mask)\n",
    "        # grid_img  = make_image_grid([comp, comp_mask])\n",
    "        # cv2.imwrite(f'./results/composite_{image_id}.jpg', grid_img)\n",
    "\n",
    "        net      = ControlComModel(device=0, model_type=\"ControlCom\")\n",
    "        comp     = net(bg_img, fg_img, bbox, fg_mask, task=['blending', 'harmonization'])\n",
    "        bg_img   = draw_bbox_on_image(bg_img, bbox)\n",
    "        grid_img = make_image_grid([bg_img, fg_img, comp[0], comp[1]])\n",
    "        cv2.imwrite('./results/controlcom_result{}.jpg'.format(image_id), grid_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c895c84d-894a-419a-b942-18a64d2a0bca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LibCom Environment",
   "language": "python",
   "name": "libcom"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
