{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82bf80cc-aee0-466c-abc0-f9136748e4ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***Python 3.8 Deprecation Notice***\n",
      "Python 3.8 will no longer be supported in new releases after October 1, 2024.\n",
      "Please upgrade to Python 3.9 or later.\n",
      "For additional details please see https://deprecation.voxel51.com\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading split 'validation' to '/home/lradovan/fiftyone/coco-2017/validation' if necessary\n",
      "Found annotations at '/home/lradovan/fiftyone/coco-2017/raw/instances_val2017.json'\n",
      "Sufficient images already downloaded\n",
      "Existing download of split 'validation' is sufficient\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***Python 3.8 Deprecation Notice***\n",
      "Python 3.8 will no longer be supported in new releases after October 1, 2024.\n",
      "Please upgrade to Python 3.9 or later.\n",
      "For additional details please see https://deprecation.voxel51.com\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing dataset 'coco-2017-validation-10'. To reload from disk, either delete the existing dataset or provide a custom `dataset_name` to use\n"
     ]
    }
   ],
   "source": [
    "import fiftyone as fo\n",
    "from fiftyone import ViewField as F\n",
    "\n",
    "# Load a small dataset\n",
    "dataset = fo.zoo.load_zoo_dataset(\n",
    "    \"coco-2017\",\n",
    "    split=\"validation\",\n",
    "    label_types=[\"detections\"],\n",
    "    max_samples=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53198e4d-0862-4f71-b590-22a2e71ec3b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "640\n",
      "save result to  ./results/grid/000000000785.jpg\n",
      "end testing!\n"
     ]
    }
   ],
   "source": [
    "from libcom import FOPAHeatMapModel\n",
    "from libcom.utils.process_image import make_image_grid, draw_bbox_on_image\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "fg_img = '/home/lradovan/workspace/libcom/tests/source/foreground/1.jpg'\n",
    "fg_mask = '/home/lradovan/workspace/libcom/tests/source/foreground_mask/1.png'\n",
    "\n",
    "#test_set = get_test_list_fopa_heatmap()\n",
    "\n",
    "test_set = []\n",
    "for sample in dataset[5:6]:\n",
    "    print(sample.metadata.width)\n",
    "    test_set.append({'foreground': fg_img, 'foreground_mask': fg_mask, 'background': sample.filepath})\n",
    "    \n",
    "result_dir = './results/'\n",
    "net = FOPAHeatMapModel(device=0)\n",
    "\n",
    "for pair in test_set[:1]:\n",
    "    fg_img, fg_mask, bg_img = pair['foreground'], pair['foreground_mask'], pair['background']\n",
    "    bboxes, heatmaps = net(fg_img, fg_mask, bg_img, cache_dir=os.path.join(result_dir, 'cache'), heatmap_dir=os.path.join(result_dir, 'heatmap'))\n",
    "    img_name  = os.path.basename(bg_img).replace('.png', '.jpg')\n",
    "    bbox = bboxes[0]\n",
    "    bg_img  = draw_bbox_on_image(bg_img, [bbox[0], bbox[1], bbox[0] + bbox[2], bbox[1] + bbox[3]])\n",
    "    grid_img  = make_image_grid([bg_img, fg_img, heatmaps[0]])\n",
    "    os.makedirs(os.path.join(result_dir, 'grid'), exist_ok=True)\n",
    "    res_path  = os.path.join(result_dir, 'grid', img_name)\n",
    "    cv2.imwrite(res_path, grid_img)\n",
    "    print('save result to ', res_path)\n",
    "print(f'end testing!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "829be8b3-4a4e-4ee8-9c0f-b17172f3d9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Gets the largest bounding box from a background image\n",
    "'''\n",
    "def get_bounding_box(sample):\n",
    "    detections = sample.ground_truth.detections\n",
    "    if len(detections) > 0:\n",
    "        print(f\"\\nImage path: {sample.filepath}\")\n",
    "        \n",
    "        # Find largest bounding box\n",
    "        largest_det = max(detections, key=lambda d: get_box_area(d.bounding_box))\n",
    "\n",
    "        # get bbox\n",
    "        bbox = largest_det.bounding_box\n",
    "        width = sample.metadata.width\n",
    "        height = sample.metadata.height\n",
    "        \n",
    "        x1 = int(bbox[0] * width)\n",
    "        y1 = int(bbox[1] * height)\n",
    "        x2 = int((bbox[0] + bbox[2]) * width)\n",
    "        y2 = int((bbox[1] + bbox[3]) * height)\n",
    "\n",
    "    return [x1, y1, x2, y2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e19cc827-9bce-4493-aa89-4db9a1047757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes a box of the form [x1, x2, y1, y2]\n",
    "def get_box_area(box):\n",
    "    return (box[1] - box[0]) * (box[3] - box[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "62859c25-6ef3-4d75-84fb-d24ba8627537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get overlap / background subject\n",
    "def overlap_ratio(occluder_bb, occludee_bb):\n",
    "\n",
    "    #top left and bottom right points\n",
    "    occluder_x1, occluder_y1, occluder_x2, occluder_y2 = occluder_bb\n",
    "    occludee_x1, occludee_y1, occludee_x2, occludee_y2 = occludee_bb\n",
    "\n",
    "    # area of the foreground object\n",
    "    occludee_area = (occludee_x2 - occludee_x1) * (occludee_y2 - occludee_y1)\n",
    "\n",
    "    # area of the background object being covered by the foreground object\n",
    "    overlap_area = max(0, min(occludee_x2, occluder_x2) - max(occludee_x1, occluder_x1)) * max(0, min(occludee_y2, occluder_y2) - max(occludee_y1, occluder_y1))\n",
    "\n",
    "    # overlap over the total background object area\n",
    "    return overlap_area / occludee_area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e40a3046-ebed-4deb-b3ce-5d75013792ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need some way to get the bounding boxes for the occluders....\n",
    "import random\n",
    "\n",
    "def get_bbox_list(bg_bbox, fg_w, fg_h):\n",
    "\n",
    "    bboxes = []\n",
    "\n",
    "    occludee_x1 = bg_bbox[0]\n",
    "    occludee_y1 = bg_bbox[1]\n",
    "    occludee_x2 = bg_bbox[2]\n",
    "    occludee_y2 = bg_bbox[3]\n",
    "\n",
    "    num_boxes = 5 # iterate over multiple boxes\n",
    "    overlap_threshold = .2 # ensure a minimum amount of overlap\n",
    "\n",
    "    # print(bg_bbox, fg_w, fg_h)\n",
    "    \n",
    "    for _ in range(num_boxes):\n",
    "        # top left corner of the occluder bounding box:\n",
    "\n",
    "        occluder_x1 = random.randint(max(0, occludee_x1 - fg_w), occludee_x2) # overlapping in the x-direction\n",
    "        occluder_y1 = random.randint(max(0, occludee_y1 - fg_h), occludee_y2) # overlapping in the y-direction\n",
    "\n",
    "        # TODO: Fix so doesnt exceed background image\n",
    "        occluder_x2 = occluder_x1 + fg_w\n",
    "        occluder_y2 = occluder_y1 + fg_h\n",
    "\n",
    "        occluder_bb = [occluder_x1, occluder_y1, occluder_x2, occluder_y2]\n",
    "        occluded_ratio = overlap_ratio(occluder_bb, bg_bbox)\n",
    "\n",
    "        if occluded_ratio >= overlap_threshold:\n",
    "            bboxes.append([occluder_x1, occluder_y1, occluder_x2, occluder_y2])\n",
    "    \n",
    "    return bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "72bb5c37-f15f-4698-a0aa-7183cc4f0210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Image path: /home/lradovan/fiftyone/coco-2017/validation/data/000000000785.jpg\n",
      "23 23\n",
      "[]\n",
      "49 47\n",
      "[]\n",
      "73 70\n",
      "[]\n",
      "98 94\n",
      "[]\n",
      "123 118\n",
      "[]\n",
      "147 141\n",
      "[[361, 51, 508, 192], [296, 245, 443, 386], [322, 67, 469, 208]]\n",
      "172 165\n",
      "[[397, 74, 569, 239], [283, 262, 455, 427], [209, 183, 381, 348]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (165,172) into shape (163,172)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 55\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28mprint\u001b[39m(bbox_list)\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m bbox \u001b[38;5;129;01min\u001b[39;00m bbox_list:\n\u001b[0;32m---> 55\u001b[0m         comp, comp_mask \u001b[38;5;241m=\u001b[39m \u001b[43mget_composite_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfg_img\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfg_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbg_img\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbbox\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m         \u001b[38;5;66;03m# bbox_score = net(comp, comp_mask)\u001b[39;00m\n\u001b[1;32m     57\u001b[0m         \u001b[38;5;66;03m# if bbox_score > score:\u001b[39;00m\n\u001b[1;32m     58\u001b[0m         \u001b[38;5;66;03m#     res = comp, comp_mask\u001b[39;00m\n\u001b[1;32m     59\u001b[0m         \u001b[38;5;66;03m#     score = bbox_score\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28mprint\u001b[39m(score)\n",
      "File \u001b[0;32m/opt/conda/envs/Libcom/lib/python3.8/site-packages/libcom/naive_composition/generate_composite_image.py:58\u001b[0m, in \u001b[0;36mget_composite_image\u001b[0;34m(foreground_image, foreground_mask, background_image, bbox, option)\u001b[0m\n\u001b[1;32m     56\u001b[0m bg_img   \u001b[38;5;241m=\u001b[39m read_image_opencv(background_image)\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m option \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 58\u001b[0m     comp_img,comp_mask \u001b[38;5;241m=\u001b[39m \u001b[43msimple_composite_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbg_img\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfg_img\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfg_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbbox\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m option \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgaussian\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     60\u001b[0m     comp_img,comp_mask \u001b[38;5;241m=\u001b[39m gaussian_composite_image(bg_img, fg_img, fg_mask, bbox)\n",
      "File \u001b[0;32m/opt/conda/envs/Libcom/lib/python3.8/site-packages/libcom/naive_composition/generate_composite_image.py:83\u001b[0m, in \u001b[0;36msimple_composite_image\u001b[0;34m(bg_img, fg_img, fg_mask, bbox)\u001b[0m\n\u001b[1;32m     81\u001b[0m x1, y1, x2, y2 \u001b[38;5;241m=\u001b[39m bbox\n\u001b[1;32m     82\u001b[0m comp_mask \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((bg_img\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], bg_img\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]), dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39muint8) \n\u001b[0;32m---> 83\u001b[0m \u001b[43mcomp_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43my1\u001b[49m\u001b[43m:\u001b[49m\u001b[43my2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx1\u001b[49m\u001b[43m:\u001b[49m\u001b[43mx2\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m fg_mask\n\u001b[1;32m     84\u001b[0m comp_img \u001b[38;5;241m=\u001b[39m bg_img\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m     85\u001b[0m comp_img[y1:y2, x1:x2]  \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(fg_mask[:,:,np\u001b[38;5;241m.\u001b[39mnewaxis]\u001b[38;5;241m>\u001b[39m \u001b[38;5;241m127\u001b[39m, fg_region, comp_img[y1:y2, x1:x2])\n",
      "\u001b[0;31mValueError\u001b[0m: could not broadcast input array from shape (165,172) into shape (163,172)"
     ]
    }
   ],
   "source": [
    "# IN PROGRESS\n",
    "# TODO: Fix broadcast issue between bounding box and mask (they are different sizes!!!)\n",
    "\n",
    "from libcom.utils.process_image import *\n",
    "from libcom.utils.environment import *\n",
    "from libcom import OPAScoreModel\n",
    "from libcom import get_composite_image\n",
    "from libcom.utils.process_image import make_image_grid\n",
    "import cv2\n",
    "import csv\n",
    "from PIL import Image\n",
    "\n",
    "net = OPAScoreModel(device=0, model_type='SimOPA')\n",
    "\n",
    "# for each scale, take the highest OPA score\n",
    "fg_scale_num = 16\n",
    "\n",
    "# trying with just one foreground image (teddy bear) and one background image (skier)\n",
    "foreground_image = '/home/lradovan/workspace/libcom/tests/source/foreground/1.jpg'\n",
    "foreground_mask = '/home/lradovan/workspace/libcom/tests/source/foreground_mask/1.png'\n",
    "\n",
    "bg_imgs = []\n",
    "for sample in dataset[5:6]:\n",
    "    bg_imgs.append([sample.filepath, get_bounding_box(sample)])\n",
    "\n",
    "background_image = bg_imgs[0][0]\n",
    "\n",
    "cache_dir = './cache'\n",
    "\n",
    "from libcom.fopa_heat_map.source.prepare_multi_fg_scales import prepare_multi_fg_scales\n",
    "scaled_fg_dir, scaled_mask_dir, csv_path = prepare_multi_fg_scales(cache_dir, foreground_image, foreground_mask, background_image, fg_scale_num)\n",
    " \n",
    "# iterate over the different foreground scales\n",
    "with open(csv_path, mode='r', newline='') as csv_file:\n",
    "    csv_reader = csv.DictReader(csv_file)\n",
    "    score = 0\n",
    "    res = [-1, -1]\n",
    "    for row in csv_reader:\n",
    "        fg_name   = '{}_{}_{}_{}.jpg'.format(row[\"fg_name\"].split(\".\")[0],row[\"bg_name\"].split(\".\")[0],int(row[\"newWidth\"]),int(row[\"newHeight\"]))\n",
    "        mask_name = '{}_{}_{}_{}.jpg'.format(row[\"fg_name\"].split(\".\")[0],row[\"bg_name\"].split(\".\")[0],int(row[\"newWidth\"]),int(row[\"newHeight\"]))\n",
    "        scale     = row['scale']\n",
    "        fg_w = int(row['newWidth'])\n",
    "        fg_h = int(row['newHeight'])\n",
    "        \n",
    "        save_name = fg_name.split(\".\")[0] + '_' + str(scale) + '.jpg'\n",
    "\n",
    "        bg_img    = read_image_pil(background_image)\n",
    "        fg_img    = read_image_pil(os.path.join(scaled_fg_dir, fg_name))\n",
    "        fg_mask   = read_mask_pil(os.path.join(scaled_mask_dir, mask_name))\n",
    "        bbox_list = get_bbox_list(bg_imgs[0][1], fg_w, fg_h)\n",
    "\n",
    "        print(fg_w, fg_h)\n",
    "        print(bbox_list)\n",
    "\n",
    "        for bbox in bbox_list:\n",
    "            comp, comp_mask = get_composite_image(fg_img, fg_mask, bg_img, bbox)\n",
    "            bbox_score = net(comp, comp_mask)\n",
    "            if bbox_score > score:\n",
    "                res = comp, comp_mask\n",
    "                score = bbox_score\n",
    "\n",
    "    print(score)\n",
    "        # grid_img  = make_image_grid(res)\n",
    "        # cv2.imwrite('/home/lradovan/results/', grid_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28f7132-500c-45c8-bf8c-5ba9ff443d71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LibCom Environment",
   "language": "python",
   "name": "libcom"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
